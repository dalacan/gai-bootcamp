{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4358d166-dc16-444a-8a41-3224fa3c0b49",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88713c8d-b3d5-4804-8809-fa6cdfae13f2",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "This notebook is based off: https://github.com/gkamradt/langchain-tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad0c20-46ac-4026-8bb9-a1b51d243789",
   "metadata": {},
   "source": [
    "## SageMaker Studio Notebook\n",
    "\n",
    "- instance: ml.g4dn.xlarge\n",
    "- kernel: python 3\n",
    "- pyTorch 1.13 Python 3.9 GPU Optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbc177a-0c1f-4a30-992d-965fefca0fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (23.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade langchain --quiet\n",
    "!pip install transformers faiss-gpu --quiet\n",
    "!pip install bs4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1474e4e8-896e-4fa1-ad82-561c7c9ad439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.27\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(langchain.__version__)\n",
    "# assert int(langchain.__version__.split(\".\")[-1]) >= 194"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be66f15-e11e-4aeb-abb1-61ae9af84f08",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb9931-08e0-4370-acea-9088320d07d7",
   "metadata": {},
   "source": [
    "Deploy from SageMaker JumpStart\n",
    "\n",
    "- textembedding-gpt-j-6b (ml.g5.12xlarge)\n",
    "- jumpstart-dft-falcon-40b-instruct-bf16 (ml.g5.48xlarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71387ff-39ef-47eb-b361-b946dc2e43bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_MODEL_CONFIG_ = {\n",
    "    \n",
    "    \"jumpstart-dft-hf-textembedding-gpt-j-6b\": {\n",
    "        \"aws_region\": \"us-east-1\",\n",
    "        \"endpoint_name\": \"jumpstart-dft-hf-textembedding-gpt-j-6b\"\n",
    "    },\n",
    "    \n",
    "    \"jumpstart-dft-hf-llm-falcon-40b-instruct-bf16\" : {\n",
    "        \"aws_region\": \"us-east-1\",\n",
    "        \"endpoint_name\": \"jumpstart-dft-hf-llm-falcon-40b-instruct-bf16\"\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3d53e-2619-4f22-94cd-1127da1075c0",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e058ae-046d-4746-af11-5005816cfcb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
    "my_text = \"What day comes after Friday?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00208a91-4220-4945-b433-b4cfc3cece66",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e70acac-8d8a-415a-be73-781cd8491603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-932cefd87595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n\u001b[1;32m      4\u001b[0m          metadata={\n\u001b[1;32m      5\u001b[0m              \u001b[0;34m'my_document_id'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m234234\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.schema'"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40919e-1a17-4302-b114-39b918529915",
   "metadata": {},
   "source": [
    "## Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bde540-ca58-4c06-9be7-c74804436f36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cca9d2-fde8-44fe-a1ce-e0886198ceb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[\n",
    "    SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "    HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7560f5d-d458-4227-bba6-b05a7e3ec790",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04e739-fdf8-4555-98e9-4b43095fcb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "\n",
    "parameters ={\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm_falcon_instruct = SagemakerEndpoint(\n",
    "    endpoint_name=_MODEL_CONFIG_[\"jumpstart-dft-hf-llm-falcon-40b-instruct-bf16\"][\"endpoint_name\"],\n",
    "    region_name=_MODEL_CONFIG_[\"jumpstart-dft-hf-llm-falcon-40b-instruct-bf16\"][\"aws_region\"],\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898ee1b-fce5-4af5-93a9-94fcd7b4f509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm_falcon_instruct(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d96b310-f31f-43f2-88e4-77e59195cfd2",
   "metadata": {},
   "source": [
    "## Text Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce36a22-b4ba-49d5-86a0-84db445cd901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "import json\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: List[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]\n",
    "\n",
    "emb_content_handler = ContentHandler()\n",
    "\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=_MODEL_CONFIG_[\"jumpstart-dft-hf-textembedding-gpt-j-6b\"][\"endpoint_name\"],\n",
    "    region_name=_MODEL_CONFIG_[\"jumpstart-dft-hf-textembedding-gpt-j-6b\"][\"aws_region\"],\n",
    "    content_handler=emb_content_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d12adf-eb2f-48ce-b77e-1440ec2e4bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Hi! It's time for the beach\"\n",
    "\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4215e75-9ea0-466a-89b8-95a0d7fe8974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_embedding = embeddings.embed_documents([text])\n",
    "print (f\"Your embedding is length {len(doc_embedding[0])}\")\n",
    "print (f\"Here's a sample: {doc_embedding[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c2d76-5094-413b-828c-c35ad1700bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_similarity_heatmap(text_labels, embeddings, rotation):\n",
    "    \"\"\"Takes sentences, embeddings and rotation as input and plot similarity heat map.\n",
    "\n",
    "    Args:\n",
    "      text_labels: a list of sentences to compute semantic textual similarity search.\n",
    "      embeddings: a list of embedding vectors, each of which corresponds to a sentence.\n",
    "      rotation: rotation used for display of the text_labels.\n",
    "    \"\"\"\n",
    "    inner_product = np.inner(embeddings, embeddings)\n",
    "    sns.set(font_scale=1.1)\n",
    "    graph = sns.heatmap(\n",
    "        inner_product,\n",
    "        xticklabels=text_labels,\n",
    "        yticklabels=text_labels,\n",
    "        vmin=np.min(inner_product),\n",
    "        vmax=1,\n",
    "        cmap=\"OrRd\",\n",
    "    )\n",
    "    graph.set_xticklabels(text_labels, rotation=rotation)\n",
    "    graph.set_title(\"Semantic Textual Similarity Between Sentences\")\n",
    "    \n",
    "sentences = [\n",
    "    # Pets\n",
    "    \"Your koala is really cute.\",\n",
    "    \"How cute your koala is!\",\n",
    "    \"You have such a cute koala!\",\n",
    "    # Location\n",
    "    \"Melbourne is the place where I work and live.\",\n",
    "    \"I work and live in Melbourne.\",\n",
    "    # Color\n",
    "    \"What color do you like the most?\",\n",
    "    \"What is your favourite color?\",\n",
    "]\n",
    "\n",
    "doc_emb = embeddings.embed_documents(sentences)\n",
    "plot_similarity_heatmap(\n",
    "    sentences, normalize(\n",
    "    np.array(doc_emb), axis=1), \n",
    "    90\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c2308-7cd1-4f4e-840a-cf179c18b5e0",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3b029-651c-4111-b845-0cb49794cb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "sm_llm_falcon_instruct(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19740785-c886-4d0c-a738-a852c505c805",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75ed87-ac02-469b-95f8-377a3088ff78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Melbourne')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {sm_llm_falcon_instruct(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9743a-d6a7-40e4-b93e-627b3dcf07e6",
   "metadata": {},
   "source": [
    "## Example Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f815262-72fb-4776-be8a-9ccec897fa86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2d2ed-8a30-41b3-b918-beec6186ae31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples, \n",
    "    \n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings, \n",
    "    \n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS, \n",
    "    \n",
    "    # This is the number of examples to produce.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefdafe4-fe41-4a99-b21e-e9992d9c102c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdb9b8-bf72-46ce-a321-803ea1ee6720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select a noun!\n",
    "# my_noun = \"students\"\n",
    "my_noun = \"flower\"\n",
    "\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b4523-bb3a-4a0a-952d-07a7c06436b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm_falcon_instruct.model_kwargs ={\n",
    "        \"max_new_tokens\": 5,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": .2,\n",
    "        \"stop\": [\"Input\", \"\\n\"]\n",
    "}\n",
    "sm_llm_falcon_instruct(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a0d2b-564a-4dd8-86d3-3758fa841a7d",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "1. **Format Instructions** - A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
    "2. **Parser** - A method which will extract your model's text output into a desired structure (usually json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9280ef-7904-4577-9289-4b7ad00145bb",
   "metadata": {},
   "source": [
    "#### No Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3580874-6605-4ed0-b275-99a4221eda0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_llm_falcon_instruct.model_kwargs ={\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": .2,\n",
    "        # \"stop\": [\"Input\", \"\\n\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8beb2b-3b53-4afd-a2e6-82f49e04cfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\"\"\",\n",
    "    input_variables=[\"user_input\"],\n",
    ")\n",
    "\n",
    "output = sm_llm_falcon_instruct(prompt.format(user_input=\"welcom to califonya!\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b88f9-72ba-4cba-af9a-d68a9ff8e696",
   "metadata": {},
   "source": [
    "## JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac95413-8abf-4e3e-bdb3-2574602b40e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# How you would like your response structured. This is basically a fancy prompt template\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# How you would like to parse your output\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# See the prompt template you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26d619-be13-48f5-9e18-d08396c24ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bc583-8171-4baa-98b1-a51728c8ee29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_output = sm_llm_falcon_instruct(promptValue)\n",
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295610bb-2ce9-4f1b-b9c3-f0329d59c305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9def72-2bbf-4451-b6de-2cfc46780662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same\n",
    "from langchain import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=sm_llm_falcon_instruct)\n",
    "output = chain.run(\"welcom to califonya!\")\n",
    "print(output)\n",
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d69ef0-e2c3-47e5-bc1a-9f878d135e4e",
   "metadata": {},
   "source": [
    "## Document Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0e555-7735-4885-a23e-c6ed3c59117a",
   "metadata": {},
   "source": [
    "### Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b35c4-739d-46ea-9cf7-5992a720e2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95802ff9-eb53-4c7d-a01b-101de70a5711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\n",
    "data = loader.load()\n",
    "\n",
    "print (f\"Found {len(data)} comments\")\n",
    "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2280b-7640-48ce-81b7-13a3d58fe9db",
   "metadata": {},
   "source": [
    "### Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd7988-49f7-4ecc-a791-d6f6ab9514c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import os\n",
    "\n",
    "# # specify the csv file path\n",
    "# csv_file = 'Amazon_SageMaker_FAQs.csv'\n",
    "\n",
    "# # specify the txt file path\n",
    "# txt_file = 'Amazon_SageMaker_FAQs.txt'\n",
    "\n",
    "# # function to convert csv to txt\n",
    "# def csv_to_txt(csv_file, txt_file):\n",
    "#     with open(csv_file, 'r') as infile, open(txt_file, 'w') as outfile:\n",
    "#         # read the csv\n",
    "#         csv_reader = csv.reader(infile)\n",
    "#         for row in csv_reader:\n",
    "#             # write to the txt file\n",
    "#             outfile.write(' '.join(row) + os.linesep)\n",
    "            \n",
    "# # call the function\n",
    "# csv_to_txt(csv_file, txt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623614db-506f-4095-a390-54e39b7273d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9aebc-1a77-423a-a7cc-046ede723349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open('Amazon_SageMaker_FAQs.txt') as f:\n",
    "    pg_faq = f.read()\n",
    "    \n",
    "print (f\"You have {len([pg_faq])} document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc89ea8-30c2-430a-8421-0902fdc3a805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_faq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f8659-1018-4fde-ad52-3cad03e64b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774edef2-9853-4eee-9814-6cfd5076d988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Preview:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e12f3-dbde-4fed-8ecd-59282f6c7bfd",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8be54b-5c7f-4992-8c2c-995507634077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "loader = TextLoader('Amazon_SageMaker_FAQs.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a08c8f-f8c3-48cf-95e0-f25011a3e8b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Get embedding engine ready\n",
    "embeddings\n",
    "\n",
    "# Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ad496-28f1-4e5a-811e-2751842f23cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init your retriever. Asking for just 1 document back\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3759c-1644-4764-838f-a2922cd3f3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"Which instance is supported for spot training?\")\n",
    "print(\"\\n\\n\".join([x.page_content for x in docs[:3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a2e4e-06e1-46d6-af7d-6c40bc1912d6",
   "metadata": {},
   "source": [
    "## VectorStores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e2251-eaaa-4f82-827a-c88879c14baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f71671-2109-4466-b7d4-f0e5a97674ee",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185f928-d359-42a1-81c8-025c0cc7cf09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881fd85-292d-4bd2-910b-a068c291cc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 1500,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1,\n",
    "    \"seed\": 123\n",
    "}\n",
    "\n",
    "sm_llm_falcon_instruct.model_kwargs = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3079d7c-852a-44f6-b272-54b736210a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Holds my 'location' chain\n",
    "location_chain = LLMChain(llm=sm_llm_falcon_instruct, prompt=prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834731f6-cf9f-44c0-8cb7-b21b39a66980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Holds my 'meal' chain\n",
    "meal_chain = LLMChain(llm=sm_llm_falcon_instruct, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebc51d-d664-4b69-bfb3-516561908ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "review = overall_chain.run(\"Melbourne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca9b01-74a0-48e2-b3cd-c7b44f0ee757",
   "metadata": {},
   "source": [
    "### Summarization Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b2812-938c-4e9b-a502-69ad9d075bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('Amazon_SageMaker_FAQs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=5)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73fde4d-faab-46a6-93be-5d7172cd2646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "sm_llm_falcon_instruct.model_kwargs = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a3723-72e1-484f-a02e-c7905e5a1386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "# chain = load_summarize_chain(sm_llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain = load_summarize_chain(sm_llm_falcon_instruct, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032134d-4bc3-4961-897f-c61358fe4e59",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "- Agents: decision maker as to the next action\n",
    "- Tools: capability of an agent\n",
    "- Toolkit: groups of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ad79c-51df-40a3-a684-19db04456240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0061687-c68f-4a93-9d71-eeb6bfde13a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.25,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": .1,\n",
    "}\n",
    "\n",
    "sm_llm_falcon_instruct.model_kwargs = parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb595d8-912e-49cb-8c9d-24fb7b1f533d",
   "metadata": {},
   "source": [
    "### Build-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766d050-f789-46d1-87d5-3cadb79fb60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools(['python_repl', 'llm-math'], llm=sm_llm_falcon_instruct)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, sm_llm_falcon_instruct, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True,)\n",
    "\n",
    "# Now let's test it out!\n",
    "# agent.run(\"\"\"\n",
    "# Write a Python script that prints \"Hello, world!\"\n",
    "# \"\"\")\n",
    "\n",
    "agent.run(\"\"\"\n",
    "What is 2.3 ^ 4.5?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dbdee0-f66a-45db-a3d8-cdaf6463039b",
   "metadata": {},
   "source": [
    "### DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014fc2b-3c13-4839-aac5-f9255f304471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser, ZeroShotAgent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "from typing import List, Union\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab664c-3dcc-4731-afd4-be6da7d6e5f8",
   "metadata": {},
   "source": [
    "#### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca06b1e-5ebb-4ed9-8875-d0530699e807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84eee37-e706-49f6-9b4d-1bb3e5548b93",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3dbc87-d8cb-4db0-845f-b2e464755aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can create the tool to pass to an agent\n",
    "python_repl = PythonREPLTool()\n",
    "print(python_repl.run(\"print('Hello'); 5\"))\n",
    "\n",
    "repl_tool = Tool(\n",
    "    name=\"Python REPL\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "    func=python_repl.run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d623ba-db85-4f4f-9bab-97e34c269d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "llm_math = LLMMathChain.from_llm(sm_llm_falcon_instruct, verbose=True)\n",
    "print(llm_math.run(\"What is 13 raised to the .3432 power?\"))\n",
    "\n",
    "math_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    description=\"Useful for when you need to answer questions about math.\",\n",
    "    func=llm_math.run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3edc3-9a05-4eab-8d4b-1c2eec729603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [math_tool, repl_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b891b4-766c-4ed3-92d8-2dc45649d79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        # print(intermediate_steps)\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\n\" # Thought:\\n\"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        print(self.template.format(**kwargs))\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "\n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5948845-37d6-4f66-a473-f3d36d6ab12f",
   "metadata": {},
   "source": [
    "#### Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664236b-857f-431d-8e5c-e383f95bcdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].split(\"\\n\")[0].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "    \n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c7138-3f03-43ff-81ad-63f98d275115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.25,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": .1,\n",
    "}\n",
    "\n",
    "sm_llm_falcon_instruct.model_kwargs = parameters\n",
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=sm_llm_falcon_instruct, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9ea1d-3999-46ce-9051-f2fa5a980c6a",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc73166-3743-4fa9-838b-7481684d0fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "print(tool_names)\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")\n",
    "\n",
    "# agent = ZeroShotAgent(llm_chain=llm_chain, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f584dcf4-63bd-4d20-9734-9e95d9486312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "# agent_executor.run('Write a Python script that prints \"Hello, world!\"')\n",
    "# agent_executor.run(\"What is 13 raised to the .3432 power?\")\n",
    "agent_executor.run(\"what is (4.5*2.1)/2.2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0113ca-fb38-4949-902d-22bd413a2314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(4.5*2.1)/2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d57bc4-54d3-4bc5-8963-b4aa50aebffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.run(\"What is the 10th fibonacci number?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc069a5-8b03-4af1-b3a5-13768e11cc92",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb4d56-d8f1-44ee-a06e-98973bc9b969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# history = ChatMessageHistory()\n",
    "\n",
    "# history.add_ai_message(\"hi!\")\n",
    "# history.add_user_message(\"what is the capital of france?\")\n",
    "# history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c48d53-cb32-47d9-9936-eb04e5682d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.chat_memory.add_user_message(\"I'm visting Sydney Australia, what should I have for dinner there?\\n\\n\")\n",
    "memory.chat_memory.add_ai_message(\"Sydney has lot of nice food, Visit China town maybe.\\n\\n\")\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3c14f-ca75-4407-95a6-484e020e9aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1.0,\n",
    "    \"stop\": [\"\\n\"]\n",
    "}\n",
    "\n",
    "sm_llm_falcon_instruct.model_kwargs = parameters\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=sm_llm_falcon_instruct, verbose=True, memory=memory,\n",
    "    prompt = PromptTemplate(template=\"\"\"\n",
    "    You are a nice AI bot that helps a user figure out what to eat in one short sentence.\n",
    "    \n",
    "    Current conversation:\n",
    "    {history}\n",
    "    Human: {input}\n",
    "    AI:\"\"\", input_variables=[\"history\", \"input\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94696638-3b1a-4471-8ab3-1ec8917171c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What about melbourne?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0222be4-d7d6-44ef-acdb-522b5a10e406",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "- Delete deployed LLM endpoint"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

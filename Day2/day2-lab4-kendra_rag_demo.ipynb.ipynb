{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96891362-46d6-4d85-9631-d64f50e5194f",
   "metadata": {},
   "source": [
    "# RAG with Kendra\n",
    "\n",
    "#### If you are running this in sagemaker studio you need to select a kernel with a python version >3.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa9b97-9abe-43bd-8868-7d90b51a3c2e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "This notebook is based on the this aws blog \n",
    "* https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/\n",
    "\n",
    "And this repo\n",
    "* https://github.com/aws-samples/amazon-kendra-langchain-extensions\n",
    "\n",
    "---\n",
    "\n",
    "### Solution overview\n",
    "The following diagram shows the architecture of a GenAI application with a RAG approach.\n",
    "\n",
    "<img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/05/02/ML-13807-image001-new.png\">\n",
    "\n",
    "We use an Amazon Kendra index to ingest enterprise unstructured data from data sources such as wiki pages, MS SharePoint sites, Atlassian Confluence, and document repositories such as Amazon S3. When a user interacts with the GenAI app, the flow is as follows:\n",
    "\n",
    "1. The user makes a request to the GenAI app.\n",
    "2. The app issues a search query to the Amazon Kendra index based on the user request.\n",
    "3. The index returns search results with excerpts of relevant documents from the ingested enterprise data.\n",
    "4. The app sends the user request and along with the data retrieved from the index as context in the LLM prompt.\n",
    "5. The LLM returns a succinct response to the user request based on the retrieved data.\n",
    "6. The response from the LLM is sent back to the user.\n",
    "\n",
    "With this architecture, you can choose the most suitable LLM for your use case. LLM options include our partners Hugging Face, AI21 Labs, Cohere, and others hosted on an Amazon SageMaker endpoint, as well as models by companies like Anthropic and OpenAI. With Amazon Bedrock, you will be able to choose Amazon Titan, Amazonâ€™s own LLM, or partner LLMs such as those from AI21 Labs and Anthropic with APIs securely without the need for your data to leave the AWS ecosystem. The additional benefits that Amazon Bedrock will offer include a serverless architecture, a single API to call the supported LLMs, and a managed service to streamline the developer workflow.\n",
    "\n",
    "For the best results, a GenAI app needs to engineer the prompt based on the user request and the specific LLM being used. Conversational AI apps also need to manage the chat history and the context. GenAI app developers can use open-source frameworks such as LangChain that provide modules to integrate with the LLM of choice, and orchestration tools for activities such as chat history management and prompt engineering. We have provided the KendraIndexRetriever class, which implements a LangChain retriever interface, which applications can use in conjunction with other LangChain interfaces such as chains to retrieve data from an Amazon Kendra index. We have also provided a few sample applications in the GitHub repo. You can deploy this solution in your AWS account using the step-by-step guide in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a935567-2d68-4ba5-887e-a8e15fbad714",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "0. [Prerequisites](#Prerequisites)\n",
    "1. [Permissions and environment variables](#1.-Permissions-and-environment-variables)\n",
    "2. [Select a pre-trained model](#2.-Select-a-pre-trained-model)\n",
    "3. [Retrieve Artifacts & Deploy an Endpoint](#3.-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "4. [Query endpoint and parse response](#4.-Query-endpoint-and-parse-response)\n",
    "5. [Query endpoint with Langchain and Kendra Index](#5.-Query-endpoint-with-Langchain-and-Kendra-Index)\n",
    "6. [[OPTIONAL] Installing Streamlet application and running a WebUI for a chatbot](#6.-[OPTIONAL]-Installing-Streamlit-application-and-running-a-WebUI-for-a-chatbot)\n",
    "7. [Clean up the endpoint](#7.-Clean-up-the-endpoint)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657a8cc-a1b3-4328-9606-9d62290715af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Kendra Index\n",
    "\n",
    "---\n",
    "\n",
    "Use the provided AWS CloudFormation to create a new Amazon Kendra index. This template includes sample data containing AWS online documentation for Amazon Kendra, Amazon Lex, and Amazon SageMaker. Alternately, if you have an Amazon Kendra index and have indexed your own dataset, you can use that. \n",
    "\n",
    "\n",
    "Deployment steps:\n",
    "   1. Download the [template](https://github.com/aws-samples/amazon-kendra-langchain-extensions/blob/main/kendra_retriever_samples/kendra-docs-index.yaml) from the github repo\n",
    "   2. Deploy it using the [cloudformation console](https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create)\n",
    "      1. Select Upload a template file and then the choose file button to select the template you just downloaded\n",
    "      2. Press Next\n",
    "   3. Provide a stack name and press Next   \n",
    "   4. Leave all default options and press Next\n",
    "   5. Check the acknowledgement at the bottom and press Submit\n",
    "   6. The stack will take around 15 minutes to deploy\n",
    "   7. Take note of the `AWSRegion` and `KendraIndexID` in the Outputs tab as we will need it in later steps   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2adb33-8b75-4769-b292-90b04d051374",
   "metadata": {},
   "source": [
    "### Kendra Permissions\n",
    "---\n",
    "\n",
    "You will need to update your SageMaker execution role with permissions to query Kendra. \n",
    "\n",
    "1. Navigate to IAM console and select Roles\n",
    "2. Search for your SageMaker execution role it will look like `AmazonSageMaker-ExecutionRole-<TIMESTAMP>`\n",
    "3. Select your role and add permission search for `AmazonKendraReadOnlyAccess` and attach the policy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d5df5-6684-4e33-b4a5-7015c11d784f",
   "metadata": {},
   "source": [
    "### AWS Langchain\n",
    "\n",
    "---\n",
    "\n",
    "This repo provides a set of utility classes to work with Langchain. It currently has a retriever class KendraIndexRetriever for working with a Kendra index and sample scripts to execute the QA chain for SageMaker, Open AI and Anthropic providers.\n",
    "\n",
    "---\n",
    "\n",
    "Clone the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a95fc-7a8f-4f44-b034-4c2ad2160974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/aws-samples/amazon-kendra-langchain-extensions.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60cd17c-c417-44f1-bccf-c2941628d801",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install the classes:\n",
    "we use a specific version of amazon-kendra-langchain-extensions as it later gets integrated into langchain and would require change of the code of this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de88b05-3a2d-4613-87a9-5a96c8bbb326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd amazon-kendra-langchain-extensions && git checkout 28cb1d4de7cf3bfe8984c6365ce248c12e8b77e0 && pip install . --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a58ac-3980-40a3-ab49-3a884b33c9ce",
   "metadata": {},
   "source": [
    "### 1. Permissions and environment variables\n",
    "\n",
    "---\n",
    "To host on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0221f42-b4de-4c97-be49-c95915a2b945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addf0b8-479f-4f7d-b420-d53e59e6480f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Select a pre-trained model\n",
    "***\n",
    "You can continue with the default model, or can choose a different model. A complete list of SageMaker pre-trained models can also be accessed at [SageMaker pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#). Be sure to select a model that can be used for text2text generation.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed3d4d-5c77-42d3-bf1d-dccc6be203b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-text2text-flan-t5-xl\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b0b66-a447-4a71-9308-8f0bd3795334",
   "metadata": {},
   "source": [
    "### 3. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "***\n",
    "\n",
    "Using SageMaker, we can perform inference on the pre-trained model, even without fine-tuning it first on a new dataset. We start by retrieving the `deploy_image_uri`, `deploy_source_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd9ef9-2bce-4da1-8ee1-51026ba9d52f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, instance_types, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "# Retrieve the inference instance type for the specified model.\n",
    "instance_type = instance_types.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version, scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# Deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c0a56-41ed-4785-bc6a-01f74a60c071",
   "metadata": {},
   "source": [
    "**If you have a predeployed endpoint you want to use you can define it here and avoid running the code block above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b002e0-bf24-45e5-9a56-fefa7e50d8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = \"<YOUR_PREDEPLOYED_ENDPOINT_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caff9f07-6101-4f53-b391-70da82c72cae",
   "metadata": {},
   "source": [
    "### 4. Query endpoint and parse response\n",
    "\n",
    "---\n",
    "Input to the endpoint is any string of text formatted as json and encoded in `utf-8` format. Output of the endpoint is a `json` with generated text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533139aa-4dbd-46f0-9e71-fdff9fad2c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint(encoded_text, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/x-text\", Body=encoded_text\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b239f8-7079-4015-b33f-67846c6abae1",
   "metadata": {},
   "source": [
    "---\n",
    "Below, we can query the model for information that it does not know.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604937dd-8df4-46d6-b455-020778ff9d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "text = \"What is Amazon Lex?\"\n",
    "\n",
    "query_response = query_endpoint(text.encode(\"utf-8\"), endpoint_name=endpoint_name)\n",
    "generated_text = parse_response(query_response)\n",
    "print(\n",
    "    f\"Inference:{newline}\"\n",
    "    f\"input text: {text}{newline}\"\n",
    "    f\"generated text: {bold}{generated_text}{unbold}{newline}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1929e8a-7076-4119-a527-7b47126e67a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Query endpoint with Langchain and Kendra Index\n",
    "\n",
    "---\n",
    "Now we will use use the `KendraIndexRetriever` retriever class with Langchain to retrieve information from our Kendra Index that matches the query.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb18f64-b235-4959-b6d5-d077b3028a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UPDATE THE FOLLOWING WITH THE OUTPUTS FROM THE CLOUDFORMATION DEPLOYMENT\n",
    "kendra_index_id=\"<YOUR_KENDRA_INDEX_ID>\"\n",
    "region=\"<YOUR_KENDRA_INDEX_DEPLOYMENY_REGION>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cc080-98b7-4bb8-990b-1b3bb5b8a1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from aws_langchain.kendra_index_retriever import KendraIndexRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "import json\n",
    "\n",
    "class ContentHandler(ContentHandlerBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_texts\"][0]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        region_name=\"us-east-1\", \n",
    "        model_kwargs={\"temperature\":1e-10, \"max_length\": 500},\n",
    "        content_handler=content_handler\n",
    "    )\n",
    "\n",
    "retriever = KendraIndexRetriever(kendraindex=kendra_index_id,\n",
    "        awsregion=region,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "The following is a friendly conversation between a human and an AI.\n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "If the AI does not know the answer to a question, it truthfully says it\n",
    "does not know.\n",
    "{context}\n",
    "Instruction: Based on the above documents, provide a detailed answer for, {question} Answer \"don't know\" if not present in the document. Solution:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa(\"What's Amazon Lex?\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}\\n\\n{bold}Sources:{unbold}')\n",
    "\n",
    "for doc in result['source_documents']:\n",
    "    print(f'''\\n{doc.metadata[\"title\"]}\\n{doc.metadata[\"source\"]}\\n{doc.metadata[\"excerpt\"]}\\n''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b10281-27da-4744-b265-00ab3066df81",
   "metadata": {},
   "source": [
    "### Example Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c20f1-9360-4637-9649-dede0feaabc8",
   "metadata": {},
   "source": [
    "The quality of the response you will recieve is going to depend on the model you have deployed. Try deploying a different model endpoint and comparing the results you recieve from the same queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3a15a-06f2-4b70-997e-558b3810a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(\"What service can I use to build a chatbot?\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc3a1b-2910-4729-807d-38031264dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(\"How can I track the health of my amazon lex box?\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc84da-93a5-4523-ac25-415fc6b38c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(\"What's the pricing for amazon Kendra?\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db909d-51dd-464b-bc9a-5b3fa4546726",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(\"What do I need for a sagemaker endpoint configuration?\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233b77e-4abd-4ea1-a0ec-5239d7d16dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(\"List the steps to deploy a sagemaker endpoint?\")\n",
    "print(f'{bold}Answer{unbold}: {result[\"result\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9c252-6851-4386-b042-653316f70075",
   "metadata": {},
   "source": [
    "### 6. [OPTIONAL] Installing Streamlit application and running a WebUI for a chatbot\n",
    "\n",
    "---\n",
    "This sections provides instructions on how to run a streamlet application within sagemaker studio and accessing it using jupyter proxy. The commands and instructions below need to be run inside a **SageMaker System Terminal**.\n",
    "\n",
    "---\n",
    "\n",
    "1. Launch a new SageMaker System Terminal \n",
    "   1. From the SageMaker Studio Home screen select `Open Launcher`\n",
    "   2. From the Launcher panel under `Utilities and files` select `System terminal`\n",
    "2. Activate the conda environment\n",
    "```\n",
    "conda activate studio\n",
    "```\n",
    "3. Install the AWS Langchain utility classes from the repo downloaded in step 1 (make sure you're in the right folder)\n",
    "```\n",
    "pip install ./amazon-kendra-langchain-extensions\n",
    "```\n",
    "4. [Optional] For executing sample chains, install the optional dependencies\n",
    "```\n",
    "pip install \"./amazon-kendra-langchain-extensions/[samples]\"\n",
    "```\n",
    "5. Set your environment variables\n",
    "```\n",
    "export AWS_REGION=\"<YOUR_AWS_REGION>\"\n",
    "export KENDRA_INDEX_ID=\"<YOUR_KENDRA_INDEX_ID>\"\n",
    "export FLAN_XL_ENDPOINT=\"<YOUR_SAGEMAKER_ENDPOINT_FOR_FLAN_T_XL>\"\n",
    "```\n",
    "6. Run the streamlit application\n",
    "```\n",
    "cd ./amazon-kendra-langchain-extensions/samples\n",
    "streamlit run app.py flanxl\n",
    "```\n",
    "7. This will output something similar to the below, you need to take note of the port (in this case 8501)\n",
    "```\n",
    "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
    "\n",
    "\n",
    "  You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Network URL: http://169.255.255.2:8501\n",
    "  External URL: http://18.213.200.192:8501\n",
    "```\n",
    "8. Copy the current URL of the SageMaker Studio which should have the form:\n",
    "```\n",
    "https://<YOUR_STUDIO_DOMAIN>.studio.<AWS_REGION>.sagemaker.aws/jupyter/default/lab/workspaces/auto-Z/tree/kendra_rag_demo.ipynb\n",
    "```\n",
    "9. Delete everything from `lab/` onwards and replace it with `proxy/<PORT>/`\n",
    "   1. DON'T FORGET THE END `/`\n",
    "```\n",
    "https://<YOUR_STUDIO_DOMAIN>.studio.<AWS_REGION>.sagemaker.aws/jupyter/default/proxy/8501/\n",
    "```\n",
    "10. Paste the new address into the browser and you will now be able to access your chatbot UI which uses Langchain and Kendra. Each response will list the sources from Kendra it used for its answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d37967-38b0-47b2-b36a-1938a1a239d3",
   "metadata": {},
   "source": [
    "### 7. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3d629-4778-4b72-b65e-2597ffa21805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

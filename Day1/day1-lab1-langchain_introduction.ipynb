{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sagemaker import Session\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Foundation Model based Applications\n",
    "\n",
    "Working with LLMs to provide you with advanced reasoning and routing capabilities is easy to get started with. After all the models understand human level instructions, and provide formattable string outputs as a result. \n",
    "\n",
    "Yet, when you are looking to develop production ready applications you will require robust data integrations to provide input to your model, you want to solve the alignment problem with LLMs, tune the behavior to your specific corporate governance and brand messaging.\n",
    "\n",
    "Complex workflows will involve multiple stages to create intermediate results. These stages require the model to switch roles, or might involve optimized task models to be more efficient, or even fine-tuned further on the specific tasks. \n",
    "\n",
    "All of this creates complexity when creating and maintaining your LLM based applications in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiiuae/falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "#load stored variables from previous notebook\n",
    "%store -r\n",
    "\n",
    "# Initialize key environment variables\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "model_version = \"*\"\n",
    "\n",
    "print(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the power of LangChain\n",
    "Recently the community unified their efforts on a high-level Framework to ease the development of foundation model based applications.\n",
    "LangChain was developed to ease the integration of models deployed, or used over proprietary APIs. It lets you easily integrate models into your application, manage the templates for prompts to tune your model behaviour, provide IO, add memory and chain multiple reasoning and action steps. \n",
    "\n",
    "### What is LangChain\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "It helps us with:\n",
    "1. **Integration** - Bring external data, such files, databases, webcontent, API data to your application\n",
    "2. **Coordination** - Develop reusable, modularized pipelines to execute complex workflows \n",
    "3. **Agency** - Enable your LLM to interact with it's environmetn via decision making"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of using the Framework\n",
    "1. Components - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
    "\n",
    "2. Customized Chains - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. Speed ðŸš¢ - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. Community ðŸ‘¥ - Wonderful discord and community support, meet ups, hackathons, etc.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting your model on AWS\n",
    "To work with your models on AWS you can use either an integration with the SageMaker endpoint, or in the future directly talk to the Bedrock API. \n",
    "\n",
    "For now, let's look at how to work with a custom SageMaker Model Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model endpoint: tiiuae/falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 10,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=_MODEL_CONFIG_[inference_model]['endpoint_name'],\n",
    "    region_name=aws_region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "\n",
    "print(f\"Loaded model endpoint: {inference_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saturday.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\">>QUESTION<<What day comes after Friday.>>ANSWER<<\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic langchain application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every LangChain application centers around your LLM model. This can be either a deployed inference endpoint, or a managed service (Bedrock, OpenAI API). The framework provides a series of out of the box integrations in the `llms` module and can be easily expanded to your use case. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "A model takes a series of messages and returns a message as output\n",
    "\n",
    "You can choose between:\n",
    "1. **LanguageModel** Takes text and returns text\n",
    "2. **Chat Model** Takes a series of messages and returns a message output\n",
    "3. **Embedding Models** Transform your text into a latent space vector to power similarity search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "A wrapper around a typical text input, text output interaction with the model. No structure is expected, and no structure is maintained. Good starting point for many non-chat applications. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we established our connection, we can query the model by sending it instructions as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are 10 names for a template factory library for prompt engineering:\n",
      "\n",
      "1. FactoryBot\n",
      "2. FactoryGirl\n",
      "3. FactoryMaker\n",
      "4. FactoryBoy\n",
      "5. Fabricator\n",
      "6. FactoryMan\n",
      "7. Builder\n",
      "8. FactoryBot\n",
      "9. FactoryBot\n",
      "10. FactoryGirl\n"
     ]
    }
   ],
   "source": [
    "text = \">>QUESTION<<Give me 10 names for a template factory library for prompt engineering. Ensure to create the required number of examples. Only provide the items of the list>>ANSWER<<\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"\n",
    ">>QUESTION<<Create a list of services a company named {prompt} could sell.\n",
    ">>ANSWER<<\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are some potential services a company named FactoryBot could sell:\n",
      "\n",
      "1. Customized software development\n",
      "2. Mobile app development\n",
      "3. Website development\n",
      "4. E-commerce development\n",
      "5. Digital marketing services\n",
      "6. Social media marketing services\n",
      "7. Search engine optimization services\n",
      "8. Pay-per-click advertising services\n",
      "9. Content marketing services\n",
      "10. Graphic design services\n",
      "11. Video production services\n",
      "12. Virtual reality development\n",
      "13. 3D printing services\n",
      "14. Prototyping services\n",
      "15. Industrial design services\n",
      "16. Manufacturing services\n",
      "17. Supply chain management services\n",
      "18. Product testing and evaluation services\n",
      "19. Technical consulting services\n",
      "20. Business consulting services. \n",
      "\n",
      "This list is not exhaustive and can be expanded or customized based on the specific needs and offerings of the company and its clients.\n"
     ]
    }
   ],
   "source": [
    "print(llm(complex_prompt.format(prompt=\"FactoryBot\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use vanilla string formatting to integrate information into our models. This allows us to pass information in a structed manner into the model, masking the general nature of the model. This allows to create all the common products you see being built natively on LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "architect_prompt = \"\"\"\n",
    "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
    "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
    "focussed. Given the following context\n",
    "\n",
    "Context:\n",
    "#System Requirements:\n",
    "{requirements}\n",
    "#Scale:\n",
    "{scale}\n",
    "#Features:\n",
    "{features}\n",
    ">>QUESTION<<Describe an architecture on AWS in technical detail\n",
    ">>ANSWER<<\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
      "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
      "focussed. Given the following context\n",
      "\n",
      "Context:\n",
      "#System Requirements:\n",
      "A website for my foodstore\n",
      "#Scale:\n",
      "Must handle 10k requests per second in peak. Must be globally available. Must be reponsive and fast\n",
      "#Features:\n",
      "Landing page describing our product. About page describing the company. Career page describing open positions.\n",
      ">>QUESTION<<Describe an architecture on AWS in technical detail\n",
      ">>ANSWER<<\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = architect_prompt.format(\n",
    "    requirements=\"A website for my foodstore\", \n",
    "    scale=\"Must handle 10k requests per second in peak. Must be globally available. Must be reponsive and fast\", \n",
    "    features=\"Landing page describing our product. About page describing the company. Career page describing open positions.\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To meet the requirements of the foodstore website, an architecture on AWS can be designed as follows:\n",
      "\n",
      "1. Web Application: Use Amazon Elastic Compute Cloud (EC2) to launch web servers that can handle incoming requests. Use Amazon Elastic Load Balancer (ELB) to distribute incoming requests across multiple web servers. Use Amazon Relational Database Service (RDS) to store the website's data. Use Amazon S3 to store images and other static files for the website.\n",
      "\n",
      "2. Content Delivery Network (CDN): Use Amazon CloudFront to serve the website's content from edge locations around the globe. This can significantly improve the website's responsiveness and availability.\n",
      "\n",
      "3. Application Load Balancer (ALB): Use Amazon Application Load Balancer to distribute incoming requests to multiple EC2 instances. This can ensure that the website stays available even during high traffic spikes.\n",
      "\n",
      "4. Security: Use Amazon Web Application Firewall \n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but can get a bit clunky when you try to scale it out to more complex use cases. The next type of model wrapper provides a solution to this problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "These models structure their input and outputs with Schemata that enable you to reason about the expected input and output process. This helps to build more complex designs by seperating the inputs used to provide the model with its role instruction, the query and the context to the query. \n",
    "\n",
    "Currently this is only implemented for API based models such as ChatGTP and Anthropic.\n",
    "\n",
    "This section is OPTIONAL, as you will have to create your own ChatGPT API key to follow along. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Could not import openai python package. Please install it with `pip install openai`. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 3\u001b[0m chat_llm \u001b[39m=\u001b[39mChatOpenAI(openai_api_key\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msk-OseEMn7iwYhQPodrX1MuT3BlbkFJrxZH5jdPHRgeL6Lw0aIV\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/load/serializable.py:61\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Could not import openai python package. Please install it with `pip install openai`. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_llm =ChatOpenAI(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m \u001b[39mimport\u001b[39;00m HumanMessage, SystemMessage, AIMessage\n\u001b[0;32m----> 2\u001b[0m chat_llm([\n\u001b[1;32m      3\u001b[0m     SystemMessage(content\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are an unhelpful AI bot that makes jokes at whatever the user says.\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m     HumanMessage(content\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mI would like to go to New York, how should i do this?\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m     AIMessage(content\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m???\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat_llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "chat_llm([\n",
    "    SystemMessage(content=\"You are an unhelpful AI bot that makes jokes at whatever the user says.\"),\n",
    "    HumanMessage(content=\"I would like to go to New York, how should i do this?\"),\n",
    "    AIMessage(content=\"???\")\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemata\n",
    "\n",
    "We see that ChatModels use typed classes to structure inputs. This is an example of a LangChain `Schema`, but its just one of many. \n",
    "\n",
    "LangChain currently provides the following schemata:\n",
    "\n",
    "* **Text** The primary interface to interact with a model (used with LanguageModels\n",
    "* **ChatMessages** What you saw we defined up with the ChatModel\n",
    "* **Examples** Input/output pairs acting as context for fine tuning model behavior in n-shot learning\n",
    "* **Document** Piece of unstructured data holding data as content and metadata for retrieval in context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatMessages Schema\n",
    "The primary interface through which end users interact with these is a chat interface. For this reason, some model providers even started providing access to the underlying API in a way that expects chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "hum_msg = HumanMessage(content='inputs send to the model by the user', additional_kwargs={}, example=True)\n",
    "hum_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = SystemMessage(content='Instructions to the model', additional_kwargs={})\n",
    "sys_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg = AIMessage(content='Context answer providing further input to the model', additional_kwargs={})\n",
    "ai_msg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure allows us to simply pass multiple requests into a model for batch processing, making application integration easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate completions for multiple sets of messages\n",
    "batch_messages = [\n",
    "    [   SystemMessage(content=\"You are a helpful assistant that translates English to German.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to German.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Examples\n",
    "These can be inputs/outputs for a model or for a chain. Both types of examples serve a different purpose. Examples for a model can be used to finetune a model. Examples for a chain can be used to evaluate the end-to-end chain, or maybe even train a model to replace that whole chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add code on how to use the examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Documents\n",
    "A piece of unstructured data. Consists of page_content (the content of the data) and metadata (auxiliary pieces of information describing attributes of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add code on how to use Documents "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1\n",
    "We are working to enable our marketing team to provide customized sales emails at scale. You are asked to create to engineer a prompt for a custom marketing email copy creation pipeline. \n",
    "\n",
    "You will be given the following inputs that are collected on the users in your database:\n",
    "* Name \n",
    "* Age\n",
    "* Interest (List of strings)\n",
    "\n",
    "You will also be given a recommended product to personalize-recommend to the user\n",
    "* Product described as a dictionary of attributes (document from DB)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work to complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "#TODO Rewrite for callcenter use case\n",
    "\n",
    "# Complete the function \n",
    "def create_email_copy(name: str, age: int, interests: List[str], product: dict) -> str:\n",
    "    \"\"\"\n",
    "    The email should be personalized, be age appropriate, target the interests \n",
    "    of the person and market the product you are selling. \n",
    "\n",
    "    Fill in this template using string formatting and a combination of the prompt\n",
    "    engineering techniques you have learned previously. \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the product you are selling. Play with the level of detail\n",
    "\n",
    "product = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of users to generate eamils for\n",
    "users = [\n",
    "    {\n",
    "    \"name\": \"\",\n",
    "    \"age\": 0,\n",
    "    \"intesrests\": [],\n",
    "    \"product\": product\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your marketing output\n",
    "for user in users:\n",
    "    print(\"\\n\\n\")\n",
    "    print(llm(create_email_copy(user)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building more complex scenarios, managing the parameters placed into the templates can be too complex for simple string injection methods. Eventually you want to describe the interface in a more programmatic way. Here the `PromptTemplate` helps to define verified input variables to be utilized in the format string.\n",
    "\n",
    "### The PromptTemplate class \n",
    "\n",
    "Let's structure our architecture template to make it reusable in our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# First we can define an exposed parameter interface to the format string\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"requirements\", \"scale\", \"features\"],\n",
    "    template=architect_prompt,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template can be asked to format itself, returning the compiled format string for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = architect_prompt.format(\n",
    "    requirements=\"External facing web application written in Javascript, global deployment\",\n",
    "    scale=\"Average of 500 requests per minute, scale events up to 3000 requests per second\",\n",
    "    features=\"Mobile website, desktop version, javascript\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(final_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a string output is nice and dandy, but what if we want to create structure returns for further use in our applications. For example, how would we continue working with an extracted set of attributes from a text in a parser scenario? \n",
    "\n",
    "Is a string good enough, or would we rather want to return a named tuple, dict or list of class instances from the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=\"\"\"\n",
    "    You identify named entities in the text and extract relations amongst them. \n",
    "    You do not answer questions, and you do not ask questions.\n",
    "    It is very important to extract all references you find. Do not skip any in your output.\n",
    "\n",
    "    # Examples:\n",
    "    The Dow Jones closed with a plus of 1456 points // (\"Dow Jones\", \"closed\", \"1456 points\")\n",
    "\n",
    "    Q: {text} // \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(references(\"Mister Higgins bought the old house next to the woods.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement the ParserClass\n",
    "class ReferenceOutputParser(BaseOutputParser):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchaining "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex workflows will involve the abbility of the model to react to specific subsets of tasks with specialized sequence of behaviors. We would capture these subsections of our application into modules called `chains`. \n",
    "\n",
    "Each chain structures the interaction with a model through specialized prompts, optional examples and optional output parsers. \n",
    "\n",
    "The langchain chains module contains the classes to help us easily create specialized sequences of chains that can receive inputs from previous model inferences as structured outputs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LLMChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "    Create a marketable company name for a company selling {product}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answering multiple questions at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'product': \"Kids kites\"},\n",
    "    {'product': \"Running shoes\"},\n",
    "    {'product': \"Tennis sports wear\"},\n",
    "]\n",
    "\n",
    "res = chain.generate(qs)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a more realistic problem to solve with LLMs that we can not easily solve with a simngle prompt. Here we see the power of chaining a series of steps conducted by an agent using serialized IO of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "    \"\"\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: First create name of the company, then provide a slogan, then create a mission and vision statement\n",
    "\n",
    "slogan_prompt = PromptTemplate(\n",
    "    input_variables=['product', 'company_name'],\n",
    "    template=\"\"\"\n",
    "    You are desigining a corporate identity for {company_name}.\n",
    "    The company is selling {product} and wants to be \n",
    "    \"\"\"\n",
    ")\n",
    "# Slogan chain\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "Let's bring it all together to create a custom LangChain that can build up a cooking article for our new online magazine. \n",
    "\n",
    "We will develop a series of chains to expand on a set of initial travel destinations to cover. \n",
    "\n",
    "Each article will contain:\n",
    "* A overview section of the travel destination\n",
    "* A list of things to do in the region\n",
    "* A recipe for a famous local dish (description, list of ingredients, step by step instruction)\n",
    "\n",
    "### Target\n",
    "Your system should create the articles based on the following inputs:\n",
    "\n",
    "* Travel destination (City, Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by creating the subchains\n",
    "\n",
    "# 1. Create the destination_overview chain\n",
    "\n",
    "\n",
    "# 2. Create the recommended_activities chain\n",
    "\n",
    "\n",
    "# 3. Create the famous_local_dish recommender \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the chains should be constructed for a subset of the workflow that is self contained. Extending too many tasks at once can overload the model and lead to poor results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now unifiy the elements and ensure the information flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of locations to cover in your reports\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the chain against the location_list\n",
    "result = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have seen how LangChain provides us with a series of abstractions on the core building blocks of LLM based applications. \n",
    "\n",
    "We have learned to connect our models, create custom Templates, use Schemata to structure our message flow, chain a series of steps and structure the models outputs to further pass into downstream systems. \n",
    "\n",
    "Each component of the system is under continued development and likely you will see the library change as it continues to mature. \n",
    "\n",
    "Still, we believe LangChain to be a useful abstraction to develop faster and build a more sustainable code base. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

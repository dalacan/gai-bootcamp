{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sagemaker import Session\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in ./.venv/lib/python3.11/site-packages (0.3.26)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: pandas>=1.3 in ./.venv/lib/python3.11/site-packages (from chromadb) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.28 in ./.venv/lib/python3.11/site-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.10.9)\n",
      "Requirement already satisfied: hnswlib>=0.7 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.7.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.6.4)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.8.1)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.98.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.24.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (4.6.3)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (3.2.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.11/site-packages (from chromadb) (1.15.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.11/site-packages (from chromadb) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.11/site-packages (from chromadb) (4.65.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.11/site-packages (from chromadb) (7.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.5.7)\n",
      "Requirement already satisfied: urllib3>=1.26 in ./.venv/lib/python3.11/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.16)\n",
      "Requirement already satisfied: pytz in ./.venv/lib/python3.11/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\n",
      "Requirement already satisfied: zstandard in ./.venv/lib/python3.11/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\n",
      "Requirement already satisfied: lz4 in ./.venv/lib/python3.11/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./.venv/lib/python3.11/site-packages (from fastapi>=0.85.1->chromadb) (0.27.0)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.3->chromadb) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: click>=7.0 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./.venv/lib/python3.11/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.7.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install chromadb tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Foundation Model based Applications\n",
    "\n",
    "Working with LLMs to provide you with advanced reasoning and routing capabilities is easy to get started with. After all the models understand human level instructions, and provide formattable string outputs as a result. \n",
    "\n",
    "Yet, when you are looking to develop production ready applications you will require robust data integrations to provide input to your model, you want to solve the alignment problem with LLMs, tune the behavior to your specific corporate governance and brand messaging.\n",
    "\n",
    "Complex workflows will involve multiple stages to create intermediate results. These stages require the model to switch roles, or might involve optimized task models to be more efficient, or even fine-tuned further on the specific tasks. \n",
    "\n",
    "All of this creates complexity when creating and maintaining your LLM based applications in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiiuae/falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "#load stored variables from previous notebook\n",
    "%store -r\n",
    "\n",
    "# Initialize key environment variables\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "model_version = \"*\"\n",
    "\n",
    "print(inference_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering your Third-Party API key (PURELY OPTIONAL!)\n",
    "We will be running a section on the ChatModel API exposed by a series of API endpoint providers such as OpenAI, Anthropic, Google Vertex. As this is currently not supported by the SageMaker deployed models, you can choose to experimment with at your own costs if you register for an OpenAI key, or you have previous access to Anthropic (as they are currently not accepting new registrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key=\"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\"\n",
    "anthropic_api_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in ./.venv/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing reuqired dependencies for third party Foundation APIs\n",
    "import sys\n",
    "if openai_api_key:\n",
    "    !{sys.executable} -m pip install openai\n",
    "if anthropic_api_key:\n",
    "    !{sys.executable} -m pip install anthropic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Widgets used across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Select, Text\n",
    "\n",
    "# This creates the widgets used across the notebook for easier configuration\n",
    "model_selections = ['SageMaker-Falcon40B']\n",
    "# Subset based on available ApiKeys\n",
    "if openai_api_key:\n",
    "    model_selections.append('OpenAI')\n",
    "if anthropic_api_key:\n",
    "    model_selections.append('Anthropic-Claude')\n",
    "\n",
    "model_selection_widget = Select(\n",
    "    options=model_selections\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model_selections = []\n",
    "if openai_api_key:\n",
    "    chat_model_selections.append('ChatOpenAI')\n",
    "if anthropic_api_key:\n",
    "    chat_model_selections.append('ChatAnthropic')\n",
    "\n",
    "chat_model_selection_widget = Select(\n",
    "    options=chat_model_selections\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the power of LangChain\n",
    "Recently the community unified their efforts on a high-level Framework to ease the development of foundation model based applications.\n",
    "LangChain was developed to ease the integration of models deployed, or used over proprietary APIs. It lets you easily integrate models into your application, manage the templates for prompts to tune your model behaviour, provide IO, add memory and chain multiple reasoning and action steps. \n",
    "\n",
    "### What is LangChain\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "It helps us with:\n",
    "1. **Integration** - Bring external data, such files, databases, webcontent, API data to your application\n",
    "2. **Coordination** - Develop reusable, modularized pipelines to execute complex workflows \n",
    "3. **Agency** - Enable your LLM to interact with it's environmetn via decision making"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of using the Framework\n",
    "1. Components - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
    "\n",
    "2. Customized Chains - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. Speed 🚢 - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. Community 👥 - Wonderful discord and community support, meet ups, hackathons, etc.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting your model on AWS\n",
    "To work with your models on AWS you can use either an integration with the SageMaker endpoint, or in the future directly talk to the Bedrock API. \n",
    "\n",
    "For now, let's look at how to work with a custom SageMaker Model Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model endpoint: tiiuae/falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "# Set model configuration\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "# Instantiate all available models \n",
    "sm_llm = SagemakerEndpoint(\n",
    "endpoint_name=_MODEL_CONFIG_[inference_model]['endpoint_name'],\n",
    "region_name=aws_region,\n",
    "model_kwargs=parameters,\n",
    "content_handler=content_handler,\n",
    ")\n",
    "\n",
    "print(f\"Loaded model endpoint: {inference_model}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an proprietary API endpoint such as OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(success) - Successfully connected to OpenAI\n",
      "You have not provided an AnthropicAPI key, and you won't have access to work with the model in this notebook\n"
     ]
    }
   ],
   "source": [
    "# Connecting to Third-party endpoints using provided API keys \n",
    "from langchain import OpenAI\n",
    "\n",
    "if openai_api_key:\n",
    "    openai_llm = OpenAI(openai_api_key=openai_api_key)\n",
    "    print(\"(success) - Successfully connected to OpenAI\")\n",
    "else:\n",
    "    print(\"(failure) - You have not provided an OpenAPI key, and you won't have access to work with the model in this notebook\")\n",
    "\n",
    "# Work with Anthropic\n",
    "from langchain import Anthropic\n",
    "\n",
    "if anthropic_api_key:\n",
    "    anthropic_llm = Anthropic(anthropic_api_key=anthropic_api_key)\n",
    "    print(\"(success) - Successfully connected to Anthropic\")\n",
    "else:\n",
    "    print(\"You have not provided an AnthropicAPI key, and you won't have access to work with the model in this notebook\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select your model\n",
    "To showcase you how different the models behave to prompting you can choose to select between an OpenSource Leaderboard model `Falcon-40B-Instruct Model` and `OpenAIs Davinci Model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ec77f7c4bf4f418dbbc23789d2c921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(options=('SageMaker-Falcon40B', 'OpenAI'), value='SageMaker-Falcon40B')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated OpenAI\n"
     ]
    }
   ],
   "source": [
    "match model_selection_widget.value:\n",
    "    case \"SageMaker-Falcon40B\":\n",
    "        llm = sm_llm\n",
    "    case \"OpenAI\":\n",
    "        llm = openai_llm\n",
    "        \n",
    "print(f\"Activated {model_selection_widget.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saturday\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"What day comes after Friday\").strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic langchain application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every LangChain application centers around your LLM model. This can be either a deployed inference endpoint, or a managed service (Bedrock, OpenAI API). The framework provides a series of out of the box integrations in the `llms` module and can be easily expanded to your use case. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "A model takes a series of messages and returns a message as output\n",
    "\n",
    "You can choose between:\n",
    "1. **LanguageModel** Takes text and returns text\n",
    "2. **Chat Model** Takes a series of messages and returns a message output\n",
    "3. **Embedding Models** Transform your text into a latent space vector to power similarity search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "A wrapper around a typical text input, text output interaction with the model. No structure is expected, and no structure is maintained. Good starting point for many non-chat applications. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we established our connection, we can query the model by sending it instructions as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "1. Template Forge \n",
      "2. Starter Kit \n",
      "3. Pattern Library \n",
      "4. Template Repository \n",
      "5. Template Hub \n",
      "6. Template Canyon \n",
      "7. Template Junction \n",
      "8. Template Factory \n",
      "9. Template Station \n",
      "10. Template Haven\n"
     ]
    }
   ],
   "source": [
    "text = \"Give me 10 names for a template factory library for prompt engineering. Ensure to create the required number of examples. Only provide the items of the list\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"\n",
    "Create a list of services a company named {prompt} could sell.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Industrial Automation Solutions\n",
      "2. Robotic Assembly Systems\n",
      "3. Automated Material Handling Solutions\n",
      "4. Intelligent Vision Systems\n",
      "5. Automated Guided Vehicle (AGV) Solutions\n",
      "6. 3D Printing Services\n",
      "7. CNC Machining Services\n",
      "8. Industrial 3D Modeling and Simulation\n",
      "9. Custom Machine Design and Fabrication\n",
      "10. Automation System Integration Services\n"
     ]
    }
   ],
   "source": [
    "print(llm(complex_prompt.format(prompt=\"FactoryBot\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use vanilla string formatting to integrate information into our models. This allows us to pass information in a structed manner into the model, masking the general nature of the model. This allows to create all the common products you see being built natively on LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "architect_prompt = \"\"\"\n",
    "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
    "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
    "focussed. Given the following context\n",
    "\n",
    "Context:\n",
    "#System Requirements:\n",
    "{requirements}\n",
    "#Scale:\n",
    "{scale}\n",
    "#Features:\n",
    "{features}\n",
    "Describe an architecture on AWS in technical detail.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
      "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
      "focussed. Given the following context\n",
      "\n",
      "Context:\n",
      "#System Requirements:\n",
      "A website for my foodstore\n",
      "#Scale:\n",
      "Must handle 10k requests per second in peak. Must be globally available. Must be reponsive and fast\n",
      "#Features:\n",
      "Landing page describing our product. About page describing the company. Career page describing open positions.\n",
      "Describe an architecture on AWS in technical detail.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = architect_prompt.format(\n",
    "    requirements=\"A website for my foodstore\", \n",
    "    scale=\"Must handle 10k requests per second in peak. Must be globally available. Must be reponsive and fast\", \n",
    "    features=\"Landing page describing our product. About page describing the company. Career page describing open positions.\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An appropriate architecture for this requirement on AWS would include the following components: \n",
      "\n",
      "1. Amazon EC2: This provides the compute resources required to host the website. We can use Auto Scaling to ensure that the servers can handle the peak load.\n",
      "\n",
      "2. Amazon S3: This provides the storage for the static website content such as HTML files, images, and videos.\n",
      "\n",
      "3. Amazon CloudFront: This provides the global content delivery network and caching for the website. It will ensure that the content is delivered quickly and reliably, even at peak load.\n",
      "\n",
      "4. Amazon Route 53: This provides the DNS services required to map the domain name to the website.\n",
      "\n",
      "5. Amazon CloudWatch: This provides the monitoring and logging services required to ensure the website is running correctly.\n",
      "\n",
      "6. Amazon Security Services: This provides the security services such as WAF, IAM and VPC to ensure the website is secure.\n",
      "\n",
      "7. Amazon RDS: This provides the relational database services required to store and process the data for the website.\n",
      "\n",
      "We can also use AWS services such as Lambda and API Gateway to provide serverless functions and APIs that can be used to extend the website. \n",
      "\n",
      "Finally, we\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but can get a bit clunky when you try to scale it out to more complex use cases. The next type of model wrapper provides a solution to this problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "These models structure their input and outputs with Schemata that enable you to reason about the expected input and output process. This helps to build more complex designs by seperating the inputs used to provide the model with its role instruction, the query and the context to the query. \n",
    "\n",
    "Currently this is only implemented for API based models such as ChatGTP and Anthropic.\n",
    "\n",
    "This section is OPTIONAL, as you will have to have your own ChatAntrophic API key to follow along. Currently registration for API keys is closed as they roll out the service. If you do not have a key yet, just read through the outputs of the notebook for reference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec678759c3e84fd0b665c5f4a279d350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(options=('ChatOpenAI',), value='ChatOpenAI')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model_selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated ChatOpenAI as chat_llm\n"
     ]
    }
   ],
   "source": [
    "# Load selected ChatModel Endpoint\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "match chat_model_selection_widget.value:\n",
    "    case \"ChatOpenAI\":\n",
    "        chat_llm = ChatOpenAI(openai_api_key=openai_api_key) \n",
    "    case \"OpenAI\":\n",
    "        llm = ChatAnthropic(anthropic_api_key=anthropic_api_key)\n",
    "        \n",
    "print(f\"Activated {chat_model_selection_widget.value} as chat_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, sorry, I can't hear you over the sound of my circuits. Did you say you want to go to Old York? Because that's much easier, just hop on a time machine and go back a few centuries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "response = chat_llm([\n",
    "    SystemMessage(content=\"You are an unhelpful AI bot that makes jokes at whatever the user says.\"),\n",
    "    HumanMessage(content=\"I would like to go to New York, how should i do this?\"),\n",
    "    AIMessage(content=\"???\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemata\n",
    "\n",
    "We see that ChatModels use typed classes to structure inputs. This is an example of a LangChain `Schema`, but its just one of many. \n",
    "\n",
    "LangChain currently provides the following schemata:\n",
    "\n",
    "* **Text** The primary interface to interact with a model (used with LanguageModels\n",
    "* **ChatMessages** What you saw we defined up with the ChatModel\n",
    "* **Examples** Input/output pairs acting as context for fine tuning model behavior in n-shot learning\n",
    "* **Document** Piece of unstructured data holding data as content and metadata for retrieval in context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatMessages Schema\n",
    "The primary interface through which end users interact with these is a chat interface. For this reason, some model providers even started providing access to the underlying API in a way that expects chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='inputs send to the model by the user', additional_kwargs={}, example=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "hum_msg = HumanMessage(content='inputs send to the model by the user', additional_kwargs={}, example=True)\n",
    "hum_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='Instructions to the model', additional_kwargs={})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg = SystemMessage(content='Instructions to the model', additional_kwargs={})\n",
    "sys_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Context answer providing further input to the model', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = AIMessage(content='Context answer providing further input to the model', additional_kwargs={})\n",
    "ai_msg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure allows us to simply pass multiple requests into a model for batch processing, making application integration easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate completions for multiple sets of messages\n",
    "batch_messages = [\n",
    "    [   SystemMessage(content=\"You are a helpful assistant that translates English to German.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to malay.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='Was für ein wundervoller Tag wir am Strand hatten, spät im Sommer.', generation_info=None, message=AIMessage(content='Was für ein wundervoller Tag wir am Strand hatten, spät im Sommer.', additional_kwargs={}, example=False))], [ChatGeneration(text='Apa satu hari yang indah yang kita lalui di pantai pada musim lewat ini.', generation_info=None, message=AIMessage(content='Apa satu hari yang indah yang kita lalui di pantai pada musim lewat ini.', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 75, 'completion_tokens': 40, 'total_tokens': 115}, 'model_name': 'gpt-3.5-turbo'}, run=RunInfo(run_id=UUID('878168d3-4f86-4a58-9fa6-0bcbb2ec1d21')))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_llm.generate(batch_messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1\n",
    "We are working to enable our marketing team to provide customized sales emails at scale. You are asked to create to engineer a prompt for a custom marketing email copy creation pipeline. \n",
    "\n",
    "You will be given the following inputs that are collected on the users in your database:\n",
    "* Name \n",
    "* Age\n",
    "* Interest (List of strings)\n",
    "\n",
    "You will also be given a recommended product to personalize-recommend to the user\n",
    "* Product described as a dictionary of attributes (document from DB)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work to complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "#TODO Rewrite for callcenter use case\n",
    "\n",
    "# Complete the function \n",
    "def create_email_copy(name: str, age: int, interests: List[str], product: dict) -> str:\n",
    "    \"\"\"\n",
    "    The email should be personalized, be age appropriate, target the interests \n",
    "    of the person and market the product you are selling. \n",
    "\n",
    "    Fill in this template using string formatting and a combination of the prompt\n",
    "    engineering techniques you have learned previously. \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the product you are selling. Play with the level of detail\n",
    "\n",
    "_product = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of users to generate eamils for\n",
    "users = [\n",
    "    {\n",
    "    \"name\": \"\",\n",
    "    \"age\": 0,\n",
    "    \"intesrests\": [],\n",
    "    \"product\": _product\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your marketing output\n",
    "for user in users:\n",
    "    print(\"\\n\\n\")\n",
    "    print(llm(create_email_copy(user)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building more complex scenarios, managing the parameters placed into the templates can be too complex for simple string injection methods. Eventually you want to describe the interface in a more programmatic way. Here the `PromptTemplate` helps to define verified input variables to be utilized in the format string.\n",
    "\n",
    "### The PromptTemplate class \n",
    "\n",
    "Let's structure our architecture template to make it reusable in our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
      "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
      "focussed. Given the following context\n",
      "\n",
      "Context:\n",
      "#System Requirements:\n",
      "{requirements}\n",
      "#Scale:\n",
      "{scale}\n",
      "#Features:\n",
      "{features}\n",
      "Describe an architecture on AWS in technical detail.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(architect_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# First we can define an exposed parameter interface to the format string\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"requirements\", \"scale\", \"features\"],\n",
    "    template=architect_prompt,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template can be asked to format itself, returning the compiled format string for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
      "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
      "focussed. Given the following context\n",
      "\n",
      "Context:\n",
      "#System Requirements:\n",
      "External facing web application written in Javascript, global deployment\n",
      "#Scale:\n",
      "Average of 500 requests per minute, scale events up to 3000 requests per second\n",
      "#Features:\n",
      "Mobile website, desktop version, javascript\n",
      "Describe an architecture on AWS in technical detail.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_prompt = architect_prompt.format(\n",
    "    requirements=\"External facing web application written in Javascript, global deployment\",\n",
    "    scale=\"Average of 500 requests per minute, scale events up to 3000 requests per second\",\n",
    "    features=\"Mobile website, desktop version, javascript\"\n",
    ")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA well-architected solution on AWS for this external facing web application would include the following components:\\n\\n1. Amazon EC2 for hosting the web application. Amazon EC2 provides the flexibility to scale up and down as the demands of the web application changes. EC2 instances should be configured for high availability and fault tolerance, with multiple Availability Zones for disaster recovery.\\n\\n2. Amazon S3 for hosting static assets such as images and other files. This will help reduce the load on the EC2 instances, and make the web application more scalable.\\n\\n3. Amazon CloudFront for distributing content to end-users. CloudFront will help improve performance and reduce latency.\\n\\n4. Amazon Route 53 for managing DNS and routing traffic.\\n\\n5. Amazon CloudWatch for monitoring the health and performance of the web application.\\n\\n6. Amazon Lambda for running serverless code in response to events. Lambda will be used to run code for scaling the web application when the number of requests exceeds a certain threshold.\\n\\n7. Amazon API Gateway for creating and managing APIs. API Gateway will be used to connect the web application with backend services and other APIs.\\n\\n8. Amazon Elastic Load Balancing for distributing traffic across multiple'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(final_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a string output is nice and dandy, but what if we want to create structure returns for further use in our applications. For example, how would we continue working with an extracted set of attributes from a text in a parser scenario? \n",
    "\n",
    "Is a string good enough, or would we rather want to return a named tuple, dict or list of class instances from the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "    You identify named entities in the text and extract relations amongst them. \n",
    "    You do not answer questions, and you do not ask questions.\n",
    "    It is very important to extract all references you find. Each referenc contains (Subject, relationship, value). \n",
    "    Do not skip any in your output.\n",
    "    {format_instructions}\n",
    "\n",
    "    # Examples:\n",
    "    The Dow Jones closed with a plus of 1456 points // [(\"Dow Jones\", \"closed\", \"1456 points\")]\n",
    "    The ./ is a relative path and assumes that you are currently in your virtual environment directory // [(\"./\", \"is\", \"relative path\")]\n",
    "    Q: {text} // \n",
    "    \"\"\"\n",
    "reference_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['text', 'format_instructions'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    [(\"Putting in effort\", \"means\", \"going beyond what\\'s required to solve problems\"), (\"Cuban\", \"said\", \"You take the initiative, and exhaust every possible option to find answers\")]'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Putting in effort means going beyond what’s required to solve problems, even when you aren’t asked to — on top of your job’s normal responsibilities, Cuban said. You take the initiative, and exhaust every possible option to find answers.\"\n",
    "llm(reference_template.format(text=text, format_instructions=\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "When we need to validate the output of a model to a given prompt or return a value as a program language object instead of a plain string we can use output parsers.\n",
    "\n",
    "The class implements a dual interface:\n",
    "1. It standardizes prompt engineering to align the output of the model to the required format\n",
    "2. It parses the resulting model output into the desired language primitive (list, dict, object)\n",
    "\n",
    "The parser classes available allow to create `pydantic` schema objects that can integrate validation steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List parser\n",
    "Like the most common scenario is to handle a return of multiple items in a list. We want to prompt the model into returning the elements in a nicely formatted structure. Typically this will be a CSV format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Introduction to Python programming\\n2. Common Python Libraries and their use cases \\n3. Getting Started with Python Data Science \\n4. Object-Oriented Programming with Python \\n5. Building Web Applications with Python \\n6. Working with Python and Databases\\n7. Debugging and Troubleshooting Python Code\\n8. Building AI and Machine Learning Projects with Python\\n9. Natural Language Processing with Python \\n10. Automating Tasks with Python Scripts'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List generating prompt\n",
    "topic_recommender_prompt=\"List {number} topics to write on blog posts about {topic}\"\n",
    "\n",
    "recommend_topic_prompt = PromptTemplate(\n",
    "    template=topic_recommender_prompt,\n",
    "    input_variables=['topic', 'number']\n",
    ")\n",
    "\n",
    "llm(recommend_topic_prompt.format(topic=\"Python\", number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser, Li\n",
    "parsed_recommender_prompt = topic_recommender_prompt + \"\\n{format_instructions}\"\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "parsed_recommender_template = PromptTemplate(\n",
    "    template=parsed_recommender_prompt,\n",
    "    input_variables=['topic', 'number'],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 10 topics to write on blog posts about Generative AI\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "gen_prompt = parsed_recommender_template.format(topic='Generative AI', number=10)\n",
    "print(gen_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Introduction to Generative AI\\n2. Applications of Generative AI\\n3. Benefits of Generative AI\\n4. Challenges of Generative AI\\n5. Generative AI Use Cases\\n6. Generative AI in Business\\n7. Generative AI in Healthcare\\n8. Generative AI in Education\\n9. Generative AI in Automotive\\n10. Generative AI for Robotics'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm(gen_prompt)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom parser\n",
    "For any entity model that relies on structure information to be returned we will have to implement a custom model based on the `BaseModel` from the `pydantic` library.\n",
    "\n",
    "The model class describes the expected schema and an optional set of validation functions to ensure the accepted values are properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from typing import List\n",
    "\n",
    "# Define the target structure\n",
    "class Entity(BaseModel):\n",
    "    subject: str = Field(description=\"subject of the relation\")\n",
    "    object: str = Field(description=\"object of the relation\")\n",
    "    relation: str = Field(description=\"relation between subject and object\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go back to our entity extraction use case\n",
    "extraction_template = \"\"\"\n",
    "You are extracting relations between entities in a text.\n",
    "You extract them in the format (Subject, Predicate, Object).\n",
    "All relations are in present tense.\n",
    "{format_instructions}\n",
    "Input: The play followed the story of Edalaine. She was a young woman.The play was written by Edgar Allan Poe.\n",
    "[('play', 'followed', 'story'), ('woman', is', 'young'), ('play', 'was written', 'poe')]\n",
    "\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_template = PromptTemplate(\n",
    "    template = extraction_template,\n",
    "    input_variables=['text'],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],\n",
       " 'kwargs': {'template': \"\\nYou are extracting relations between entities in a text.\\nYou extract them in the format (Subject, Predicate, Object).\\nAll relations are in present tense.\\n{format_instructions}\\nInput: The play followed the story of Edalaine. She was a young woman.The play was written by Edgar Allan Poe.\\n[('play', 'followed', 'story'), ('woman', is', 'young'), ('play', 'was written', 'poe')]\\n\\n{text}\\n\",\n",
       "  'input_variables': ['text'],\n",
       "  'partial_variables': {'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"subject\": {\"title\": \"Subject\", \"description\": \"subject of the relation\", \"type\": \"string\"}, \"object\": {\"title\": \"Object\", \"description\": \"object of the relation\", \"type\": \"string\"}, \"relation\": {\"title\": \"Relation\", \"description\": \"relation between subject and object\", \"type\": \"string\"}}, \"required\": [\"subject\", \"object\", \"relation\"]}\\n```'},\n",
       "  'template_format': 'f-string'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parsed_template.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"subject\": {\"title\": \"Subject\", \"description\": \"subject of the relation\", \"type\": \"string\"}, \"object\": {\"title\": \"Object\", \"description\": \"object of the relation\", \"type\": \"string\"}, \"relation\": {\"title\": \"Relation\", \"description\": \"relation between subject and object\", \"type\": \"string\"}}, \"required\": [\"subject\", \"object\", \"relation\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# The model we defined is parsed into the following context template format\n",
    "print(parsed_template.to_json()['kwargs']['partial_variables']['format_instructions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = llm(parsed_template.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOutput:\\n{\"subject\": \"you\", \"object\": \"answers\", \"relation\": \"take initiative and exhaust every possible option to find\"}'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template is not yet tuned to identify the correct subjects to extract information for. We can continue to refine the template to hit our expected target. Yet, we see that the model is producing the target format, so we can continue to parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Entity'>\n",
      "subject='you' object='answers' relation='take initiative and exhaust every possible option to find'\n"
     ]
    }
   ],
   "source": [
    "entity = parser.parse(responses)\n",
    "print(type(entity))\n",
    "print(entity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Example\n",
    "These can be inputs/outputs for a model or for a chain. Both types of examples serve a different purpose. Examples for a model can be used to finetune a model. Examples for a chain can be used to evaluate the end-to-end chain, or maybe even train a model to replace that whole chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples are structured as Q/A pairs. Let's create a list of fewshot examples for us to explore\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "    \"answer\": \n",
    "    \"\"\"\n",
    "        Are follow up questions needed here: Yes.\n",
    "        Follow up: How old was Muhammad Ali when he died?\n",
    "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "        Follow up: How old was Alan Turing when he died?\n",
    "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "        So the final answer is: Muhammad Ali\n",
    "    \"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"When was the founder of craigslist born?\",\n",
    "    \"answer\": \n",
    "    \"\"\"\n",
    "        Are follow up questions needed here: Yes.\n",
    "        Follow up: Who was the founder of craigslist?\n",
    "        Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "        Follow up: When was Craig Newmark born?\n",
    "        Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "        So the final answer is: December 6, 1952\n",
    "    \"\"\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we use a formatter to parse the examples into string inputs to our template\n",
    "example_prompt = PromptTemplate(input_variables=['question', 'answer'], template=\"Question: {question}\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "        Are follow up questions needed here: Yes.\n",
      "        Follow up: How old was Muhammad Ali when he died?\n",
      "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "        Follow up: How old was Alan Turing when he died?\n",
      "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "        So the final answer is: Muhammad Ali\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "        Are follow up questions needed here: Yes.\n",
      "        Follow up: How old was Muhammad Ali when he died?\n",
      "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "        Follow up: How old was Alan Turing when he died?\n",
      "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "        So the final answer is: Muhammad Ali\n",
      "    \n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "\n",
      "        Are follow up questions needed here: Yes.\n",
      "        Follow up: Who was the founder of craigslist?\n",
      "        Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "        Follow up: When was Craig Newmark born?\n",
      "        Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "        So the final answer is: December 6, 1952\n",
      "    \n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "# Feed the examples and formatter to FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=['input']\n",
    ")\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an example selector\n",
    "If you provide a full set of examples that cover various different topics in depth the lenght of the context can overrun the memory allocation of your model endpoint. \n",
    "\n",
    "Example selectors help to pass a subset of the examples that are relevant to the specific question at hand instead of passing the full examples.\n",
    "\n",
    "The example selector utilizes a similarity score across the embedded question and example pairs. We will cover embeddings in detail in Lab2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples most similar to the input: Who was the father of Mary Ball Washington?\n",
      "\n",
      "\n",
      "question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "answer: \n",
      "        Are follow up questions needed here: Yes.\n",
      "        Follow up: How old was Muhammad Ali when he died?\n",
      "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "        Follow up: How old was Alan Turing when he died?\n",
      "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "        So the final answer is: Muhammad Ali\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    #TODO: Replace with embedding model endpoint\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "    Chroma,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "question = \"Who was the father of Mary Ball Washington?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `example_selector` instead of a hardcoded list of examples. Under the hood this similarity search utilizes embeddings and vector stores. m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "        Are follow up questions needed here: Yes.\n",
      "        Follow up: How old was Muhammad Ali when he died?\n",
      "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "        Follow up: How old was Alan Turing when he died?\n",
      "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "        So the final answer is: Muhammad Ali\n",
      "    \n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=['input']\n",
    ")\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchaining "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex workflows will involve the abbility of the model to react to specific subsets of tasks with specialized sequence of behaviors. We would capture these subsections of our application into modules called `chains`. \n",
    "\n",
    "Each chain structures the interaction with a model through specialized prompts, optional examples and optional output parsers. \n",
    "\n",
    "The langchain chains module contains the classes to help us easily create specialized sequences of chains that can receive inputs from previous model inferences as structured outputs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LLMChain\n",
    "\n",
    "Let's play through the example of creating a product that allows users to generate a full set of marketing materials.\n",
    "\n",
    "1. Generate name proposals for a company based on intended product to be sold\n",
    "2. Generate a marketing slogan based on company values provided\n",
    "3. Create a marketing template for email communication for the new company launch\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful marketing assistant that creates a marketable company name for a company selling {product} \n",
    "    Answer with a single name only no comments or discussion.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LLMChain picks up the work for us to prompt the model with based on the template\n",
    "company_name_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"company_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Socktastic.\n"
     ]
    }
   ],
   "source": [
    "# We pass all variables required in the PromptTemplate directly into the chain\n",
    "product = \"colorful socks\"\n",
    "company_name = company_name_chain.run(product)\n",
    "print(company_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chain can generate batch predictions to answer multiple questions at a time using the `generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'product': \"Kids kites\"},\n",
    "    {'product': \"Running shoes\"},\n",
    "    {'product': \"Tennis sports wear\"},\n",
    "]\n",
    "\n",
    "company_name_generated = company_name_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KiteKids.\n",
      "\n",
      "RunShoez\n",
      "\n",
      "TennisWearX\n"
     ]
    }
   ],
   "source": [
    "for response in company_name_generated.generations:\n",
    "    print(response[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a more realistic problem to solve with LLMs that we can not easily solve with a simngle prompt. Here we see the power of chaining a series of steps conducted by an agent using serialized IO of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\", \"product\"],\n",
    "    template=\"Create a list of services and products that {company_name} can develop around {product}. The list must be commma seperated such as (abc, def, ghi, jkl)\",\n",
    ")\n",
    "services_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=services_prompt,\n",
    "    output_key=\"services\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Design Services, Printing Services, Wholesale Distribution, Custom Packaging, Online Shopping Platform, Custom Sock Designs, Subscription Services, Soaking Services, Charitable Donation Programs, Promotional Events, Gift Boxes.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: We get the recommended list of services \n",
    "services = services_chain.run(product=product, company_name=company_name)\n",
    "print(services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's create recommendations for the company slogan\n",
    "slogan_prompt = PromptTemplate(\n",
    "    input_variables=['product','company_name','services'],\n",
    "    template=\"\"\"\n",
    "    Context:\n",
    "    You are desigining a corporate identity for {company_name} selling {product}\n",
    "    The company is providing {services}. \n",
    "    Create a slogan for the company that is unique and memorable.\n",
    "    \"\"\"\n",
    ")\n",
    "# Slogan chain\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt,\n",
    "    output_key=\"slogan\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Sock It To 'Em with Socktastic!\"\n"
     ]
    }
   ],
   "source": [
    "slogan = slogan_chain.run(company_name=company_name, product=product, services=services)\n",
    "print(slogan)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining the chains \n",
    "Now that we defined the stages of our chain, identified the inputs and outputs we require and engineered our prompt templates to get the desired outputs, we can build an integrated end-to-end chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "marketing_chain = SequentialChain(\n",
    "    chains=[company_name_chain, services_chain, slogan_chain],\n",
    "    input_variables=['product'],\n",
    "    output_variables=['company_name', 'services', 'slogan']\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': 'Kites',\n",
       " 'company_name': '\\nKiteFrenzy',\n",
       " 'services': '\\n\\nKite Design, Kite Manufacturing, Kite Repair, Kite Accessories, Kite Flying Lessons, Kite Rental Services, Kite Festival Organization, Kite Festival Merchandise, Kite Flying Competitions, Kite Flying Tours, Kite Flying Videos, Kite Flying Photography, Kite Flying Clubs, Kite Flying Books, Kite Flying Magazines.',\n",
       " 'slogan': '\\n\"Let the Wind Take You on an Adventure with KiteFrenzy!\"'}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing_chain(\"Kites\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we could improve\n",
    "You will likely see that your models won't always comply with your formatting rules you try to put into their context instructions. You can further expand on this chain design by implementing the output parsers between the stages to clean up the intermediary results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "Let's bring it all together to create a custom LangChain that can build up a cooking article for our new online magazine. \n",
    "\n",
    "We will develop a series of chains to expand on a set of initial travel destinations to cover. \n",
    "\n",
    "Each article will contain:\n",
    "* A overview section of the travel destination\n",
    "* A list of things to do in the region\n",
    "* A recipe for a famous local dish (description, list of ingredients, step by step instruction)\n",
    "\n",
    "### Target\n",
    "Your system should create the articles based on the following inputs:\n",
    "\n",
    "* Travel destination (City, Country)\n",
    "\n",
    "### Focus on this exercise\n",
    "* Apply modularity to the steps in your workflow\n",
    "* Consider quality gates when chaining values across steps\n",
    "* Think about suitable prompt engineering methods to improve your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by creating the subchains\n",
    "\n",
    "# 1. Create the destination_overview chain\n",
    "\n",
    "\n",
    "# 2. Create the recommended_activities chain\n",
    "\n",
    "\n",
    "# 3. Create the famous_local_dish recommender \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the chains should be constructed for a subset of the workflow that is self contained. Extending too many tasks at once can overload the model and lead to poor results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now unifiy the elements and ensure the information flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of locations to cover in your reports\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the chain against the location_list\n",
    "result = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have seen how LangChain provides us with a series of abstractions on the core building blocks of LLM based applications. \n",
    "\n",
    "We have learned to connect our models, create custom Templates, use Schemata to structure our message flow, chain a series of steps and structure the models outputs to further pass into downstream systems. \n",
    "\n",
    "Each component of the system is under continued development and likely you will see the library change as it continues to mature. \n",
    "\n",
    "Still, we believe LangChain to be a useful abstraction to develop faster and build a more sustainable code base. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

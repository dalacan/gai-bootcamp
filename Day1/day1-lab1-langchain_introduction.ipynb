{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sagemaker import Session\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Need update to GNU C++ compiler version\n",
    "!apt-get update && apt-get install -y build-essential --quiet\n",
    "!pip install shap --quiet\n",
    "# Installs dependencies reuqired for chromadb memory vectordb and embedding library\n",
    "!pip install chromadb tiktoken langchain --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Foundation Model based Applications\n",
    "\n",
    "Working with LLMs to provide you with advanced reasoning and routing capabilities is easy to get started with. After all the models understand human level instructions, and provide formattable string outputs as a result. \n",
    "\n",
    "Yet, when you are looking to develop production ready applications you will require robust data integrations to provide input to your model, you want to solve the alignment problem with LLMs, tune the behavior to your specific corporate governance and brand messaging.\n",
    "\n",
    "Complex workflows will involve multiple stages to create intermediate results. These stages require the model to switch roles, or might involve optimized task models to be more efficient, or even fine-tuned further on the specific tasks. \n",
    "\n",
    "All of this creates complexity when creating and maintaining your LLM based applications in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load stored variables from previous notebook\n",
    "%store -r\n",
    "\n",
    "# Initialize key environment variables\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "model_version = \"*\"\n",
    "\n",
    "print(falcon_inference_model)\n",
    "print(embedding_model)\n",
    "try:\n",
    "    print(ai21_inference_model)\n",
    "except:\n",
    "    print(\"You did not load the AI21 Jurassic Grande model and will not be able to select it for this lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering your Third-Party API key (PURELY OPTIONAL!)\n",
    "We will be running a section on the ChatModel API exposed by a series of API endpoint providers such as OpenAI, Anthropic, Google Vertex. As this is currently not supported by the SageMaker deployed models, you can choose to experimment with at your own costs if you register for an OpenAI key, or you have previous access to Anthropic (as they are currently not accepting new registrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_api_key=\"\"\n",
    "anthropic_api_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installing reuqired dependencies for third party Foundation APIs\n",
    "import sys\n",
    "if openai_api_key:\n",
    "    !pip install openai --quiet\n",
    "if anthropic_api_key:\n",
    "    !pip install anthropic --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Widgets used across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Select, Text\n",
    "\n",
    "# This creates the widgets used across the notebook for easier configuration\n",
    "model_selections = [\n",
    "    'SageMaker-Falcon40B', \n",
    "    # 'AI21-Jurrasic-Grande'\n",
    "]\n",
    "# Subset based on available ApiKeys\n",
    "if openai_api_key:\n",
    "    model_selections.append('OpenAI')\n",
    "if anthropic_api_key:\n",
    "    model_selections.append('Anthropic-Claude')\n",
    "\n",
    "model_selection_widget = Select(\n",
    "    options=model_selections\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_model_selections = []\n",
    "if openai_api_key:\n",
    "    chat_model_selections.append('ChatOpenAI')\n",
    "if anthropic_api_key:\n",
    "    chat_model_selections.append('ChatAnthropic')\n",
    "\n",
    "chat_model_selection_widget = Select(\n",
    "    options=chat_model_selections\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the power of LangChain\n",
    "Recently the community unified their efforts on a high-level Framework to ease the development of foundation model based applications.\n",
    "LangChain was developed to ease the integration of models deployed, or used over proprietary APIs. It lets you easily integrate models into your application, manage the templates for prompts to tune your model behaviour, provide IO, add memory and chain multiple reasoning and action steps. \n",
    "\n",
    "### What is LangChain\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "It helps us with:\n",
    "1. **Integration** - Bring external data, such files, databases, webcontent, API data to your application\n",
    "2. **Coordination** - Develop reusable, modularized pipelines to execute complex workflows \n",
    "3. **Agency** - Enable your LLM to interact with it's environmetn via decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of using the Framework\n",
    "1. Components - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
    "\n",
    "2. Customized Chains - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. Speed ðŸš¢ - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. Community ðŸ‘¥ - Wonderful discord and community support, meet ups, hackathons, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting your model on AWS\n",
    "To work with your models on AWS you can use either an integration with the SageMaker endpoint, or in the future directly talk to the Bedrock API. For now, let's look at how to work with a custom SageMaker Model Endpoint.\n",
    "\n",
    "### Configuring the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model configuration for falcon 40b instruct\n",
    "falcon_parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}\n",
    "\n",
    "# Configuration for Jurassic Grande Model\n",
    "j2_grande_parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "\n",
    "class InferenceContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "class EmbeddingsContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        return embeddings\n",
    "\n",
    "    \n",
    "inference_content_handler = InferenceContentHandler()\n",
    "embeddings_content_handler = EmbeddingsContentHandler()\n",
    "\n",
    "# Instantiate all available models\n",
    "falcon_sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=_MODEL_CONFIG_[falcon_inference_model]['endpoint_name'],\n",
    "    region_name=aws_region,\n",
    "    model_kwargs=falcon_parameters,\n",
    "    content_handler=inference_content_handler,\n",
    "    )\n",
    "print(f\"Loaded model endpoint: {falcon_inference_model}\")\n",
    "embedding_sm_llm = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=_MODEL_CONFIG_[embedding_model]['endpoint_name'],\n",
    "    region_name=aws_region,\n",
    "    content_handler=embeddings_content_handler,\n",
    "    )\n",
    "print(f\"Loaded model endpoint: {embedding_model}\")\n",
    "\n",
    "# Optional load\n",
    "# j2_grande_sm_llm = SagemakerEndpoint(\n",
    "#     endpoint_name=_MODEL_CONFIG_[ai21_inference_model]['endpoint_name'],\n",
    "#     region_name=aws_region,\n",
    "#     model_kwargs=j2_grande_parameters,\n",
    "#     content_handler=inference_content_handler,\n",
    "#     )\n",
    "# print(f\"Loaded model endpoint: {ai21_inference_model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an proprietary API endpoint such as OpenAI\n",
    "To baseline the performance of a model on an industry wide benchmark you can also execute your code against proprietary services. This is enabled through custom connector classes that rely only on the configuration of your api_keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connecting to Third-party endpoints using provided API keys \n",
    "from langchain import OpenAI\n",
    "\n",
    "if openai_api_key:\n",
    "    openai_llm = OpenAI(openai_api_key=openai_api_key)\n",
    "    print(\"(success) - Successfully connected to OpenAI\")\n",
    "else:\n",
    "    print(\"(failure) - You have not provided an OpenAPI key, and you won't have access to work with the model in this notebook\")\n",
    "\n",
    "# Work with Anthropic\n",
    "from langchain import Anthropic\n",
    "\n",
    "if anthropic_api_key:\n",
    "    anthropic_llm = Anthropic(anthropic_api_key=anthropic_api_key)\n",
    "    print(\"(success) - Successfully connected to Anthropic\")\n",
    "else:\n",
    "    print(\"(failure) - You have not provided an AnthropicAPI key, and you won't have access to work with the model in this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select your model\n",
    "To showcase you how different the models behave to prompting you can choose to select between an OpenSource Leaderboard model `Falcon-40B-Instruct Model` and `OpenAIs Davinci Model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "match model_selection_widget.value:\n",
    "    case \"SageMaker-Falcon40B\":\n",
    "        llm = falcon_sm_llm\n",
    "    case \"AI21-Jurrasic-Grande\":\n",
    "        llm = j2_grande_sm_llm\n",
    "    case \"OpenAI\":\n",
    "        llm = openai_llm\n",
    "        \n",
    "print(f\"Activated {model_selection_widget.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm(\"What day comes after Friday\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic langchain application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every LangChain application centers around your LLM model. This can be either a deployed inference endpoint, or a managed service (Bedrock, OpenAI API). The framework provides a series of out of the box integrations in the `llms` module and can be easily expanded to your use case. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "A model takes a series of messages and returns a message as output\n",
    "\n",
    "You can choose between:\n",
    "1. **LanguageModel** Takes text and returns text\n",
    "2. **Chat Model** Takes a series of messages and returns a message output\n",
    "3. **Embedding Models** Transform your text into a latent space vector to power similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "A wrapper around a typical text input, text output interaction with the model. No structure is expected, and no structure is maintained. Good starting point for many non-chat applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we established our connection, we can query the model by sending it instructions as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Give me 10 names for a template factory library for prompt engineering. Ensure to create the required number of examples. Only provide the items of the list\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"\n",
    "Create a list of services a company named {prompt} could sell.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm(complex_prompt.format(prompt=\"FactoryBot\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use vanilla string formatting to integrate information into our models. This allows us to pass information in a structed manner into the model, masking the general nature of the model. This allows to create all the common products you see being built natively on LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "architect_prompt = \"\"\"\n",
    "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
    "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
    "focussed. Given the following context\n",
    "\n",
    "Context:\n",
    "#System Requirements:\n",
    "{requirements}\n",
    "#Scale:\n",
    "{scale}\n",
    "#Features:\n",
    "{features}\n",
    "Describe an architecture on AWS in technical detail.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = architect_prompt.format(\n",
    "    requirements=\"A website for my foodstore\", \n",
    "    scale=\"Must handle 10k requests per second in peak. Must be globally available. Must be reponsive and fast\", \n",
    "    features=\"Landing page describing our product. About page describing the company. Career page describing open positions.\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but can get a bit clunky when you try to scale it out to more complex use cases. The next type of model wrapper provides a solution to this problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "These models structure their input and outputs with Schemata that enable you to reason about the expected input and output process. This helps to build more complex designs by seperating the inputs used to provide the model with its role instruction, the query and the context to the query. \n",
    "\n",
    "Currently this is only implemented for API based models such as ChatGTP and Anthropic.\n",
    "\n",
    "This section is OPTIONAL, as you will have to have your own ChatAntrophic API key to follow along. Currently registration for API keys is closed as they roll out the service. If you do not have a key yet, just read through the outputs of the notebook for reference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_model_selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load selected ChatModel Endpoint\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "match chat_model_selection_widget.value:\n",
    "    case \"ChatOpenAI\":\n",
    "        chat_llm = ChatOpenAI(openai_api_key=openai_api_key) \n",
    "    case \"OpenAI\":\n",
    "        llm = ChatAnthropic(anthropic_api_key=anthropic_api_key)\n",
    "        \n",
    "print(f\"Activated {chat_model_selection_widget.value} as chat_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "response = chat_llm([\n",
    "    SystemMessage(content=\"You are an unhelpful AI bot that makes jokes at whatever the user says.\"),\n",
    "    HumanMessage(content=\"I would like to go to New York, how should i do this?\"),\n",
    "    AIMessage(content=\"???\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemata\n",
    "\n",
    "We see that ChatModels use typed classes to structure inputs. This is an example of a LangChain `Schema`, but its just one of many. \n",
    "\n",
    "LangChain currently provides the following schemata:\n",
    "\n",
    "* **Text** The primary interface to interact with a model (used with LanguageModels\n",
    "* **ChatMessages** What you saw we defined up with the ChatModel\n",
    "* **Examples** Input/output pairs acting as context for fine tuning model behavior in n-shot learning\n",
    "* **Document** Piece of unstructured data holding data as content and metadata for retrieval in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatMessages Schema\n",
    "The primary interface through which end users interact with these is a chat interface. For this reason, some model providers even started providing access to the underlying API in a way that expects chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "hum_msg = HumanMessage(content='inputs send to the model by the user', additional_kwargs={}, example=True)\n",
    "hum_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys_msg = SystemMessage(content='Instructions to the model', additional_kwargs={})\n",
    "sys_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ai_msg = AIMessage(content='Context answer providing further input to the model', additional_kwargs={})\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure allows us to simply pass multiple requests into a model for batch processing, making application integration easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate completions for multiple sets of messages\n",
    "batch_messages = [\n",
    "    [   SystemMessage(content=\"You are a helpful assistant that translates English to German.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to malay.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm.generate(batch_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1\n",
    "We are working to enable our marketing team to provide customized sales emails at scale. You are asked to create to engineer a prompt for a custom marketing email copy creation pipeline. \n",
    "\n",
    "You will be given the following inputs that are collected on the users in your database:\n",
    "* Name \n",
    "* Age\n",
    "* Interest (List of strings)\n",
    "\n",
    "You will also be given a recommended product to personalize-recommend to the user\n",
    "* Product described as a dictionary of attributes (document from DB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work to complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "#TODO Rewrite for callcenter use case\n",
    "\n",
    "# Complete the function \n",
    "def create_email_copy(name: str, age: int, interests: List[str], product: dict) -> str:\n",
    "    \"\"\"\n",
    "    The email should be personalized, be age appropriate, target the interests \n",
    "    of the person and market the product you are selling. \n",
    "\n",
    "    Fill in this template using string formatting and a combination of the prompt\n",
    "    engineering techniques you have learned previously. \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the product you are selling. Play with the level of detail\n",
    "\n",
    "_product = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a set of users to generate eamils for\n",
    "users = [\n",
    "    {\n",
    "    \"name\": \"\",\n",
    "    \"age\": 0,\n",
    "    \"intesrests\": [],\n",
    "    \"product\": _product\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your marketing output\n",
    "for user in users:\n",
    "    print(\"\\n\\n\")\n",
    "    print(llm(create_email_copy(user)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building more complex scenarios, managing the parameters placed into the templates can be too complex for simple string injection methods. Eventually you want to describe the interface in a more programmatic way. Here the `PromptTemplate` helps to define verified input variables to be utilized in the format string.\n",
    "\n",
    "### The PromptTemplate class \n",
    "\n",
    "Let's structure our architecture template to make it reusable in our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(architect_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# First we can define an exposed parameter interface to the format string\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"requirements\", \"scale\", \"features\"],\n",
    "    template=architect_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template can be asked to format itself, returning the compiled format string for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_prompt = architect_prompt.format(\n",
    "    requirements=\"External facing web application written in Javascript, global deployment\",\n",
    "    scale=\"Average of 500 requests per minute, scale events up to 3000 requests per second\",\n",
    "    features=\"Mobile website, desktop version, javascript\"\n",
    ")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a string output is nice and dandy, but what if we want to create structure returns for further use in our applications. For example, how would we continue working with an extracted set of attributes from a text in a parser scenario? \n",
    "\n",
    "Is a string good enough, or would we rather want to return a named tuple, dict or list of class instances from the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "    You identify named entities in the text and extract relations amongst them. \n",
    "    You do not answer questions, and you do not ask questions.\n",
    "    It is very important to extract all references you find. Each referenc contains (Subject, relationship, value). \n",
    "    Do not skip any in your output.\n",
    "    {format_instructions}\n",
    "\n",
    "    # Examples:\n",
    "    The Dow Jones closed with a plus of 1456 points // [(\"Dow Jones\", \"closed\", \"1456 points\")]\n",
    "    The ./ is a relative path and assumes that you are currently in your virtual environment directory // [(\"./\", \"is\", \"relative path\")]\n",
    "    Q: {text} // \n",
    "    \"\"\"\n",
    "reference_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['text', 'format_instructions'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Putting in effort means going beyond whatâ€™s required to solve problems, even when you arenâ€™t asked to â€” on top of your jobâ€™s normal responsibilities, Cuban said. You take the initiative, and exhaust every possible option to find answers.\"\n",
    "llm(reference_template.format(text=text, format_instructions=\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "When we need to validate the output of a model to a given prompt or return a value as a program language object instead of a plain string we can use output parsers.\n",
    "\n",
    "The class implements a dual interface:\n",
    "1. It standardizes prompt engineering to align the output of the model to the required format\n",
    "2. It parses the resulting model output into the desired language primitive (list, dict, object)\n",
    "\n",
    "The parser classes available allow to create `pydantic` schema objects that can integrate validation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List parser\n",
    "Like the most common scenario is to handle a return of multiple items in a list. We want to prompt the model into returning the elements in a nicely formatted structure. Typically this will be a CSV format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List generating prompt\n",
    "topic_recommender_prompt=\"List {number} topics to write on blog posts about {topic}\"\n",
    "\n",
    "recommend_topic_prompt = PromptTemplate(\n",
    "    template=topic_recommender_prompt,\n",
    "    input_variables=['topic', 'number']\n",
    ")\n",
    "\n",
    "llm(recommend_topic_prompt.format(topic=\"Python\", number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "parsed_recommender_prompt = topic_recommender_prompt + \"\\n{format_instructions}\"\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "parsed_recommender_template = PromptTemplate(\n",
    "    template=parsed_recommender_prompt,\n",
    "    input_variables=['topic', 'number'],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_prompt = parsed_recommender_template.format(topic='Generative AI', number=10)\n",
    "print(gen_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = llm(gen_prompt)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom parser\n",
    "For any entity model that relies on structure information to be returned we will have to implement a custom model based on the `BaseModel` from the `pydantic` library.\n",
    "\n",
    "The model class describes the expected schema and an optional set of validation functions to ensure the accepted values are properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from typing import List\n",
    "\n",
    "# Define the target structure\n",
    "class Entity(BaseModel):\n",
    "    subject: str = Field(description=\"subject of the relation\")\n",
    "    object: str = Field(description=\"object of the relation\")\n",
    "    relation: str = Field(description=\"relation between subject and object\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's go back to our entity extraction use case\n",
    "extraction_template = \"\"\"\n",
    "You are extracting relations between entities in a text.\n",
    "You extract them in the format (Subject, Predicate, Object).\n",
    "All relations are in present tense.\n",
    "{format_instructions}\n",
    "Input: The play followed the story of Edalaine. She was a young woman.The play was written by Edgar Allan Poe.\n",
    "[('play', 'about', 'Edalaine'), ('Edalaine', 'age', 'young'), ('Edalaine', 'gender', 'female'), ('play', 'written by', 'Edgar Allan Poe')]\n",
    "\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_template = PromptTemplate(\n",
    "    template = extraction_template,\n",
    "    input_variables=['text'],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(parsed_template.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The model we defined is parsed into the following context template format\n",
    "print(parsed_template.to_json()['kwargs']['partial_variables']['format_instructions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = llm(parsed_template.format(text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template is not yet tuned to identify the correct subjects to extract information for. We can continue to refine the template to hit our expected target. Yet, we see that the model is producing the target format, so we can continue to parse the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entity = parser.parse(responses)\n",
    "print(type(entity))\n",
    "print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Example\n",
    "These can be inputs/outputs for a model or for a chain. Both types of examples serve a different purpose. Examples for a model can be used to finetune a model. Examples for a chain can be used to evaluate the end-to-end chain, or maybe even train a model to replace that whole chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examples are structured as Q/A pairs. Let's create a list of fewshot examples for us to explore\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "    \"answer\": \n",
    "    \"\"\"\n",
    "        Are follow up questions needed here: Yes.\n",
    "        Follow up: How old was Muhammad Ali when he died?\n",
    "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "        Follow up: How old was Alan Turing when he died?\n",
    "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "        So the final answer is: Muhammad Ali\n",
    "    \"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"When was the founder of craigslist born?\",\n",
    "    \"answer\": \n",
    "    \"\"\"\n",
    "        Are follow up questions needed here: Yes.\n",
    "        Follow up: Who was the founder of craigslist?\n",
    "        Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "        Follow up: When was Craig Newmark born?\n",
    "        Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "        So the final answer is: December 6, 1952\n",
    "    \"\"\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then we use a formatter to parse the examples into string inputs to our template\n",
    "example_prompt = PromptTemplate(input_variables=['question', 'answer'], template=\"Question: {question}\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "# Feed the examples and formatter to FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=['input']\n",
    ")\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an example selector\n",
    "If you provide a full set of examples that cover various different topics in depth the lenght of the context can overrun the memory allocation of your model endpoint. \n",
    "\n",
    "Example selectors help to pass a subset of the examples that are relevant to the specific question at hand instead of passing the full examples.\n",
    "\n",
    "The example selector utilizes a similarity score across the embedded question and example pairs. We will cover embeddings in detail in Lab2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embeddings=embedding_sm_llm,\n",
    "    vectorstore=Chroma,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "question = \"Who was the father of Mary Ball Washington?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `example_selector` instead of a hardcoded list of examples. Under the hood this similarity search utilizes embeddings and vector stores. m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=['input']\n",
    ")\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchaining "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex workflows will involve the abbility of the model to react to specific subsets of tasks with specialized sequence of behaviors. We would capture these subsections of our application into modules called `chains`. \n",
    "\n",
    "Each chain structures the interaction with a model through specialized prompts, optional examples and optional output parsers. \n",
    "\n",
    "The langchain chains module contains the classes to help us easily create specialized sequences of chains that can receive inputs from previous model inferences as structured outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LLMChain\n",
    "\n",
    "Let's play through the example of creating a product that allows users to generate a full set of marketing materials.\n",
    "\n",
    "1. Generate name proposals for a company based on intended product to be sold\n",
    "2. Generate a marketing slogan based on company values provided\n",
    "3. Create a marketing template for email communication for the new company launch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful marketing assistant that creates a marketable company name for a company selling {product} \n",
    "    Answer with a single name only no comments or discussion.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The LLMChain picks up the work for us to prompt the model with based on the template\n",
    "company_name_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"company_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We pass all variables required in the PromptTemplate directly into the chain\n",
    "product = \"colorful socks\"\n",
    "company_name = company_name_chain.run(product)\n",
    "print(company_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chain can generate batch predictions to answer multiple questions at a time using the `generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'product': \"Kids kites\"},\n",
    "    {'product': \"Running shoes\"},\n",
    "    {'product': \"Tennis sports wear\"},\n",
    "]\n",
    "\n",
    "company_name_generated = company_name_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for response in company_name_generated.generations:\n",
    "    print(response[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a more realistic problem to solve with LLMs that we can not easily solve with a simngle prompt. Here we see the power of chaining a series of steps conducted by an agent using serialized IO of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "services_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\", \"product\"],\n",
    "    template=\"Create a list of services and products that {company_name} can develop around {product}. The list must be commma seperated such as (abc, def, ghi, jkl)\",\n",
    ")\n",
    "services_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=services_prompt,\n",
    "    output_key=\"services\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: We get the recommended list of services \n",
    "services = services_chain.run(product=product, company_name=company_name)\n",
    "print(services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Next let's create recommendations for the company slogan\n",
    "slogan_prompt = PromptTemplate(\n",
    "    input_variables=['product','company_name','services'],\n",
    "    template=\"\"\"\n",
    "    Context:\n",
    "    You are desigining a corporate identity for {company_name} selling {product}\n",
    "    The company is providing {services}. \n",
    "    Create a slogan for the company that is unique and memorable.\n",
    "    \"\"\"\n",
    ")\n",
    "# Slogan chain\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt,\n",
    "    output_key=\"slogan\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slogan = slogan_chain.run(company_name=company_name, product=product, services=services)\n",
    "print(slogan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining the chains \n",
    "Now that we defined the stages of our chain, identified the inputs and outputs we require and engineered our prompt templates to get the desired outputs, we can build an integrated end-to-end chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "marketing_chain = SequentialChain(\n",
    "    chains=[company_name_chain, services_chain, slogan_chain],\n",
    "    input_variables=['product'],\n",
    "    output_variables=['company_name', 'services', 'slogan']\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "marketing_chain(\"Kites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we could improve\n",
    "You will likely see that your models won't always comply with your formatting rules you try to put into their context instructions. You can further expand on this chain design by implementing the output parsers between the stages to clean up the intermediary results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "Let's bring it all together to create a custom LangChain that can build up a cooking article for our new online magazine. \n",
    "\n",
    "We will develop a series of chains to expand on a set of initial travel destinations to cover. \n",
    "\n",
    "Each article will contain:\n",
    "* A overview section of the travel destination\n",
    "* A list of things to do in the region\n",
    "* A recipe for a famous local dish (description, list of ingredients, step by step instruction)\n",
    "\n",
    "### Target\n",
    "Your system should create the articles based on the following inputs:\n",
    "\n",
    "* Travel destination (City, Country)\n",
    "\n",
    "### Focus on this exercise\n",
    "* Apply modularity to the steps in your workflow\n",
    "* Consider quality gates when chaining values across steps\n",
    "* Think about suitable prompt engineering methods to improve your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by creating the subchains\n",
    "\n",
    "# 1. Create the destination_overview chain\n",
    "\n",
    "\n",
    "# 2. Create the recommended_activities chain\n",
    "\n",
    "\n",
    "# 3. Create the famous_local_dish recommender \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the chains should be constructed for a subset of the workflow that is self contained. Extending too many tasks at once can overload the model and lead to poor results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now unifiy the elements and ensure the information flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of locations to cover in your reports\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the chain against the location_list\n",
    "result = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have seen how LangChain provides us with a series of abstractions on the core building blocks of LLM based applications. \n",
    "\n",
    "We have learned to connect our models, create custom Templates, use Schemata to structure our message flow, chain a series of steps and structure the models outputs to further pass into downstream systems. \n",
    "\n",
    "Each component of the system is under continued development and likely you will see the library change as it continues to mature. \n",
    "\n",
    "Still, we believe LangChain to be a useful abstraction to develop faster and build a more sustainable code base. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

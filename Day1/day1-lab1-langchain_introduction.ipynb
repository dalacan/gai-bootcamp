{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sagemaker import Session\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Foundation Model based Applications\n",
    "\n",
    "Working with LLMs to provide you with advanced reasoning and routing capabilities is easy to get started with. After all the models understand human level instructions, and provide formattable string outputs as a result. \n",
    "\n",
    "Yet, when you are looking to develop production ready applications you will require robust data integrations to provide input to your model, you want to solve the alignment problem with LLMs, tune the behavior to your specific corporate governance and brand messaging.\n",
    "\n",
    "Complex workflows will involve multiple stages to create intermediate results. These stages require the model to switch roles, or might involve optimized task models to be more efficient, or even fine-tuned further on the specific tasks. \n",
    "\n",
    "All of this creates complexity when creating and maintaining your LLM based applications in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raglc-huggingface-textgeneration1-gpt-j-2023-06-21-09-49-17-052\n"
     ]
    }
   ],
   "source": [
    "#load stored variables from previous notebook\n",
    "%store -r\n",
    "\n",
    "# Initialize key environment variables\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "model_version = \"*\"\n",
    "\n",
    "print(inference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/xfrankx/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/bin/pip: bad interpreter: /Users/xfrankx/Documents/Coding/generativeai/gai-bootcamp/Day1/.venv/bin/python3.11: no such file or directory\n",
      "Requirement already satisfied: aiohttp==3.8.4 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (3.8.4)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Collecting anyio==3.7.0\n",
      "  Using cached anyio-3.7.0-py3-none-any.whl (80 kB)\n",
      "Collecting appnope==0.1.3\n",
      "  Using cached appnope-0.1.3-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting argon2-cffi==21.3.0\n",
      "  Using cached argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting argon2-cffi-bindings==21.2.0\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp38-abi3-macosx_10_9_universal2.whl (53 kB)\n",
      "Collecting arrow==1.2.3\n",
      "  Using cached arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "Collecting asttokens==2.2.1\n",
      "  Using cached asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (4.0.2)\n",
      "Requirement already satisfied: attrs==23.1.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (23.1.0)\n",
      "Collecting backcall==0.2.0\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.2 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (4.12.2)\n",
      "Collecting bleach==6.0.0\n",
      "  Using cached bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "Collecting boto3==1.26.153\n",
      "  Using cached boto3-1.26.153-py3-none-any.whl (135 kB)\n",
      "Collecting botocore==1.29.153\n",
      "  Using cached botocore-1.29.153-py3-none-any.whl (10.9 MB)\n",
      "Requirement already satisfied: certifi==2023.5.7 in /Users/xfrankx/Library/Python/3.11/lib/python/site-packages (from -r requirements.txt (line 16)) (2023.5.7)\n",
      "Collecting cffi==1.15.1\n",
      "  Using cached cffi-1.15.1-cp311-cp311-macosx_11_0_arm64.whl (174 kB)\n",
      "Requirement already satisfied: charset-normalizer==3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (3.1.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (2.2.1)\n",
      "Collecting comm==0.1.3\n",
      "  Using cached comm-0.1.3-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: contextlib2==21.6.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (21.6.0)\n",
      "Requirement already satisfied: dataclasses-json==0.5.8 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (0.5.8)\n",
      "Collecting debugpy==1.6.7\n",
      "  Using cached debugpy-1.6.7-py2.py3-none-any.whl (4.9 MB)\n",
      "Collecting decorator==5.1.1\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting defusedxml==0.7.1\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: dill==0.3.6 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (0.3.6)\n",
      "Collecting executing==1.2.0\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting fastjsonschema==2.17.1\n",
      "  Using cached fastjsonschema-2.17.1-py3-none-any.whl (23 kB)\n",
      "Collecting fqdn==1.5.1\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (1.3.3)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (0.2.0)\n",
      "Requirement already satisfied: idna==3.4 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata==4.13.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (4.13.0)\n",
      "Collecting ipykernel==6.23.2\n",
      "  Using cached ipykernel-6.23.2-py3-none-any.whl (152 kB)\n",
      "Collecting ipython==8.14.0\n",
      "  Using cached ipython-8.14.0-py3-none-any.whl (798 kB)\n",
      "Collecting ipython-genutils==0.2.0\n",
      "  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting ipywidgets==8.0.6\n",
      "  Using cached ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "Collecting isoduration==20.11.0\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting jedi==0.18.2\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting Jinja2==3.1.2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: jmespath==1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (1.0.1)\n",
      "Collecting jsonpointer==2.3\n",
      "  Using cached jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: jsonschema==4.17.3 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (4.17.3)\n",
      "Collecting jupyter==1.0.0\n",
      "  Using cached jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting jupyter-console==6.6.3\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting jupyter-events==0.6.3\n",
      "  Using cached jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
      "Collecting jupyter_client==8.2.0\n",
      "  Using cached jupyter_client-8.2.0-py3-none-any.whl (103 kB)\n",
      "Collecting jupyter_core==5.3.0\n",
      "  Using cached jupyter_core-5.3.0-py3-none-any.whl (93 kB)\n",
      "Collecting jupyter_server==2.6.0\n",
      "  Using cached jupyter_server-2.6.0-py3-none-any.whl (375 kB)\n",
      "Collecting jupyter_server_terminals==0.4.4\n",
      "  Using cached jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting jupyterlab-pygments==0.2.2\n",
      "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting jupyterlab-widgets==3.0.7\n",
      "  Using cached jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
      "Collecting langchain==0.0.200\n",
      "  Using cached langchain-0.0.200-py3-none-any.whl (1.0 MB)\n",
      "Collecting langchainplus-sdk==0.0.10\n",
      "  Using cached langchainplus_sdk-0.0.10-py3-none-any.whl (21 kB)\n",
      "Collecting MarkupSafe==2.1.3\n",
      "  Using cached MarkupSafe-2.1.3-cp311-cp311-macosx_10_9_universal2.whl (17 kB)\n",
      "Requirement already satisfied: marshmallow==3.19.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 56)) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum==1.5.1 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 57)) (1.5.1)\n",
      "Collecting matplotlib-inline==0.1.6\n",
      "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting mistune==2.0.5\n",
      "  Using cached mistune-2.0.5-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: multidict==6.0.4 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 60)) (6.0.4)\n",
      "Requirement already satisfied: multiprocess==0.70.14 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 61)) (0.70.14)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 62)) (1.0.0)\n",
      "Collecting nbclassic==1.0.0\n",
      "  Using cached nbclassic-1.0.0-py3-none-any.whl (10.0 MB)\n",
      "Collecting nbclient==0.8.0\n",
      "  Using cached nbclient-0.8.0-py3-none-any.whl (73 kB)\n",
      "Collecting nbconvert==7.5.0\n",
      "  Using cached nbconvert-7.5.0-py3-none-any.whl (288 kB)\n",
      "Collecting nbformat==5.9.0\n",
      "  Using cached nbformat-5.9.0-py3-none-any.whl (77 kB)\n",
      "Collecting nest-asyncio==1.5.6\n",
      "  Using cached nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting notebook==6.5.4\n",
      "  Using cached notebook-6.5.4-py3-none-any.whl (529 kB)\n",
      "Collecting notebook_shim==0.2.3\n",
      "  Using cached notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: numexpr==2.8.4 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 70)) (2.8.4)\n",
      "Collecting numpy==1.24.3\n",
      "  Using cached numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "Requirement already satisfied: openapi-schema-pydantic==1.2.4 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 72)) (1.2.4)\n",
      "Collecting overrides==7.3.1\n",
      "  Using cached overrides-7.3.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging==23.1 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 74)) (23.1)\n",
      "Requirement already satisfied: pandas==2.0.2 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 75)) (2.0.2)\n",
      "Collecting pandocfilters==1.5.0\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting parso==0.8.3\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: pathos==0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 78)) (0.3.0)\n",
      "Collecting pexpect==4.8.0\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting pickleshare==0.7.5\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting platformdirs==3.5.3\n",
      "  Using cached platformdirs-3.5.3-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pox==0.3.2 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 82)) (0.3.2)\n",
      "Requirement already satisfied: ppft==1.7.6.6 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 83)) (1.7.6.6)\n",
      "Collecting prometheus-client==0.17.0\n",
      "  Using cached prometheus_client-0.17.0-py3-none-any.whl (60 kB)\n",
      "Collecting prompt-toolkit==3.0.38\n",
      "  Using cached prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\n",
      "Requirement already satisfied: protobuf==3.20.3 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 86)) (3.20.3)\n",
      "Requirement already satisfied: protobuf3-to-dict==0.1.5 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 87)) (0.1.5)\n",
      "Collecting psutil==5.9.5\n",
      "  Using cached psutil-5.9.5-cp38-abi3-macosx_11_0_arm64.whl (246 kB)\n",
      "Collecting ptyprocess==0.7.0\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting pure-eval==0.2.2\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pycparser==2.21\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting pydantic==1.10.9\n",
      "  Using cached pydantic-1.10.9-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Collecting Pygments==2.15.1\n",
      "  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: pyrsistent==0.19.3 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 94)) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 95)) (2.8.2)\n",
      "Collecting python-json-logger==2.0.7\n",
      "  Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: pytz==2023.3 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 97)) (2023.3)\n",
      "Requirement already satisfied: PyYAML==6.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 98)) (6.0)\n",
      "Collecting pyzmq==25.1.0\n",
      "  Using cached pyzmq-25.1.0-cp311-cp311-macosx_10_15_universal2.whl (1.8 MB)\n",
      "Collecting qtconsole==5.4.3\n",
      "  Using cached qtconsole-5.4.3-py3-none-any.whl (121 kB)\n",
      "Collecting QtPy==2.3.1\n",
      "  Using cached QtPy-2.3.1-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: requests==2.31.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 102)) (2.31.0)\n",
      "Collecting rfc3339-validator==0.1.4\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting rfc3986-validator==0.1.1\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: s3transfer==0.6.1 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 105)) (0.6.1)\n",
      "Collecting sagemaker==2.165.0\n",
      "  Using cached sagemaker-2.165.0.tar.gz (803 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: schema==0.7.5 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 107)) (0.7.5)\n",
      "Collecting Send2Trash==1.8.2\n",
      "  Using cached Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: six==1.16.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 109)) (1.16.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 110)) (1.0.1)\n",
      "Collecting sniffio==1.3.0\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: soupsieve==2.4.1 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 112)) (2.4.1)\n",
      "Collecting SQLAlchemy==2.0.16\n",
      "  Using cached SQLAlchemy-2.0.16-cp311-cp311-macosx_11_0_arm64.whl (2.0 MB)\n",
      "Collecting stack-data==0.6.2\n",
      "  Using cached stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 115)) (1.7.0)\n",
      "Requirement already satisfied: tenacity==8.2.2 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 116)) (8.2.2)\n",
      "Collecting terminado==0.17.1\n",
      "  Using cached terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Collecting tinycss2==1.2.1\n",
      "  Using cached tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting tornado==6.3.2\n",
      "  Using cached tornado-6.3.2-cp38-abi3-macosx_10_9_universal2.whl (424 kB)\n",
      "Collecting traitlets==5.9.0\n",
      "  Using cached traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 121)) (0.9.0)\n",
      "Collecting typing_extensions==4.6.3\n",
      "  Using cached typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: tzdata==2023.3 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 123)) (2023.3)\n",
      "Collecting uri-template==1.2.0\n",
      "  Using cached uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting urllib3==1.26.16\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting wcwidth==0.2.6\n",
      "  Using cached wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
      "Collecting webcolors==1.13\n",
      "  Using cached webcolors-1.13-py3-none-any.whl (14 kB)\n",
      "Collecting webencodings==0.5.1\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting websocket-client==1.5.3\n",
      "  Using cached websocket_client-1.5.3-py3-none-any.whl (56 kB)\n",
      "Collecting widgetsnbextension==4.0.7\n",
      "  Using cached widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: yarl==1.9.2 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 131)) (1.9.2)\n",
      "Requirement already satisfied: zipp==3.15.0 in /opt/homebrew/lib/python3.11/site-packages (from -r requirements.txt (line 132)) (3.15.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.165.0-py2.py3-none-any.whl size=1082431 sha256=e06cdc795b43e647b20ed09889ef2cf6f3c4b445da4a0712ea0cd3833bda9848\n",
      "  Stored in directory: /Users/xfrankx/Library/Caches/pip/wheels/4d/78/bc/4fb6812977bb2fc66f4da2ac0aabd610686688e81e9f0c3316\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: webencodings, wcwidth, pure-eval, ptyprocess, pickleshare, mistune, ipython-genutils, fastjsonschema, executing, backcall, appnope, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, typing_extensions, traitlets, tornado, tinycss2, sniffio, Send2Trash, rfc3986-validator, rfc3339-validator, QtPy, pyzmq, python-json-logger, Pygments, pycparser, psutil, prompt-toolkit, prometheus-client, platformdirs, pexpect, parso, pandocfilters, overrides, numpy, nest-asyncio, MarkupSafe, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, fqdn, defusedxml, decorator, debugpy, bleach, asttokens, terminado, stack-data, SQLAlchemy, pydantic, matplotlib-inline, jupyter_core, Jinja2, jedi, comm, cffi, botocore, arrow, anyio, nbformat, langchainplus-sdk, jupyter_server_terminals, jupyter_client, isoduration, ipython, argon2-cffi-bindings, nbclient, langchain, ipykernel, boto3, argon2-cffi, sagemaker, qtconsole, nbconvert, jupyter-events, jupyter-console, ipywidgets, jupyter_server, notebook_shim, nbclassic, notebook, jupyter\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.5.1\n",
      "    Uninstalling platformdirs-3.5.1:\n",
      "      Successfully uninstalled platformdirs-3.5.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.0\n",
      "    Uninstalling numpy-1.25.0:\n",
      "      Successfully uninstalled numpy-1.25.0\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.41\n",
      "    Uninstalling SQLAlchemy-1.4.41:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.41\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.7\n",
      "    Uninstalling pydantic-1.10.7:\n",
      "      Successfully uninstalled pydantic-1.10.7\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.133\n",
      "    Uninstalling botocore-1.29.133:\n",
      "      Successfully uninstalled botocore-1.29.133\n",
      "  Attempting uninstall: langchainplus-sdk\n",
      "    Found existing installation: langchainplus-sdk 0.0.16\n",
      "    Uninstalling langchainplus-sdk-0.0.16:\n",
      "      Successfully uninstalled langchainplus-sdk-0.0.16\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.207\n",
      "    Uninstalling langchain-0.0.207:\n",
      "      Successfully uninstalled langchain-0.0.207\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.133\n",
      "    Uninstalling boto3-1.26.133:\n",
      "      Successfully uninstalled boto3-1.26.133\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.167.0\n",
      "    Uninstalling sagemaker-2.167.0:\n",
      "      Successfully uninstalled sagemaker-2.167.0\n",
      "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.3 Pygments-2.15.1 QtPy-2.3.1 SQLAlchemy-2.0.16 Send2Trash-1.8.2 anyio-3.7.0 appnope-0.1.3 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 asttokens-2.2.1 backcall-0.2.0 bleach-6.0.0 boto3-1.26.153 botocore-1.29.153 cffi-1.15.1 comm-0.1.3 debugpy-1.6.7 decorator-5.1.1 defusedxml-0.7.1 executing-1.2.0 fastjsonschema-2.17.1 fqdn-1.5.1 ipykernel-6.23.2 ipython-8.14.0 ipython-genutils-0.2.0 ipywidgets-8.0.6 isoduration-20.11.0 jedi-0.18.2 jsonpointer-2.3 jupyter-1.0.0 jupyter-console-6.6.3 jupyter-events-0.6.3 jupyter_client-8.2.0 jupyter_core-5.3.0 jupyter_server-2.6.0 jupyter_server_terminals-0.4.4 jupyterlab-pygments-0.2.2 jupyterlab-widgets-3.0.7 langchain-0.0.200 langchainplus-sdk-0.0.10 matplotlib-inline-0.1.6 mistune-2.0.5 nbclassic-1.0.0 nbclient-0.8.0 nbconvert-7.5.0 nbformat-5.9.0 nest-asyncio-1.5.6 notebook-6.5.4 notebook_shim-0.2.3 numpy-1.24.3 overrides-7.3.1 pandocfilters-1.5.0 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-3.5.3 prometheus-client-0.17.0 prompt-toolkit-3.0.38 psutil-5.9.5 ptyprocess-0.7.0 pure-eval-0.2.2 pycparser-2.21 pydantic-1.10.9 python-json-logger-2.0.7 pyzmq-25.1.0 qtconsole-5.4.3 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 sagemaker-2.165.0 sniffio-1.3.0 stack-data-0.6.2 terminado-0.17.1 tinycss2-1.2.1 tornado-6.3.2 traitlets-5.9.0 typing_extensions-4.6.3 uri-template-1.2.0 urllib3-1.26.16 wcwidth-0.2.6 webcolors-1.13 webencodings-0.5.1 websocket-client-1.5.3 widgetsnbextension-4.0.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the power of LangChain\n",
    "Recently the community unified their efforts on a high-level Framework to ease the development of foundation model based applications.\n",
    "LangChain was developed to ease the integration of models deployed, or used over proprietary APIs. It lets you easily integrate models into your application, manage the templates for prompts to tune your model behaviour, provide IO, add memory and chain multiple reasoning and action steps. \n",
    "\n",
    "### What is LangChain\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "It helps us with:\n",
    "1. **Integration** - Bring external data, such files, databases, webcontent, API data to your application\n",
    "2. **Coordination** - Develop reusable, modularized pipelines to execute complex workflows \n",
    "3. **Agency** - Enable your LLM to interact with it's environmetn via decision making"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of using the Framework\n",
    "1. Components - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
    "\n",
    "2. Customized Chains - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. Speed 🚢 - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. Community 👥 - Wonderful discord and community support, meet ups, hackathons, etc.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting your model on AWS\n",
    "To work with your models on AWS you can use either an integration with the SageMaker endpoint, or in the future directly talk to the Bedrock API. \n",
    "\n",
    "For now, let's look at how to work with a custom SageMaker Model Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model endpoint: raglc-huggingface-textgeneration1-gpt-j-2023-06-21-09-49-17-052\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 300,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 30,\n",
    "    \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8\n",
    "}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/x-text\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        print(prompt)\n",
    "        print(type(prompt))\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_texts\"][0]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=inference_model,\n",
    "    region_name=aws_region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "\n",
    "print(f\"Loaded model endpoint: {inference_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What day comes after Friday\n",
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sm_llm(\u001b[39m\"\u001b[39;49m\u001b[39mWhat day comes after Friday\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/base.py:333\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    327\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    328\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     )\n\u001b[1;32m    332\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([prompt], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    334\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    335\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[1;32m    336\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/base.py:203\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    204\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m run_manager:\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/base.py:195\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    191\u001b[0m     dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 195\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    196\u001b[0m             prompts, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    199\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/base.py:493\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    491\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    492\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 493\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    494\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    495\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    497\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/sagemaker_endpoint.py:246\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError raised by inference endpoint: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 246\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontent_handler\u001b[39m.\u001b[39;49mtransform_output(response[\u001b[39m\"\u001b[39;49m\u001b[39mBody\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m stop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# This is a bit hacky, but I can't figure out a better way to enforce\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39m# stop tokens when making calls to the sagemaker endpoint.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     text \u001b[39m=\u001b[39m enforce_stop_tokens(text, stop)\n",
      "Cell \u001b[0;32mIn[60], line 26\u001b[0m, in \u001b[0;36mContentHandler.transform_output\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform_output\u001b[39m(\u001b[39mself\u001b[39m, output: \u001b[39mbytes\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     25\u001b[0m     response_json \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(output\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m response_json[\u001b[39m\"\u001b[39;49m\u001b[39mgenerated_texts\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "sm_llm(\"What day comes after Friday\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic langchain application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every LangChain application centers around your LLM model. This can be either a deployed inference endpoint, or a managed service (Bedrock, OpenAI API). The framework provides a series of out of the box integrations in the `llms` module and can be easily expanded to your use case. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "A model takes a series of messages and returns a message as output\n",
    "\n",
    "You can choose between:\n",
    "1. **LanguageModel** Takes text and returns text\n",
    "2. **Chat Model** Takes a series of messages and returns a message output\n",
    "3. **Embedding Models** Transform your text into a latent space vector to power similarity search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "A wrapper around a typical text input, text output interaction with the model. No structure is expected, and no structure is maintained. Good starting point for many non-chat applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Utilize the local endpoint deployed\n",
    "llm = OpenAI(openai_api_key=vars['OPENAI_API_KEY'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we established our connection, we can query the model by sending it instructions as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good name for a template factory library for prompt engineering\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"\n",
    "You are working to extract information from text. You are waiting for the input without asking questions. \n",
    "You are not responding to any questions asked directly. \n",
    "\n",
    "Input: During last years primary elections, Senator Charles Ray was able to increase voter share in Wisconsin\n",
    "from 12.5% to 15.5%.\n",
    "Output: [(\"Charles Ray\", \"Position\", \"Senator\"), (\")]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(complex_prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use vanilla string formatting to integrate information into our models. This allows us to pass information in a structed manner into the model, masking the general nature of the model. This allows to create all the common products you see being built natively on LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architect_prompt = \"\"\"\n",
    "Act as a solution architect experienced on AWS. You are analysing customer requirements to create\n",
    "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
    "focussed. You are not engaging in general discussions outside of the scope of system design. \n",
    "\n",
    "#System Requirements:\n",
    "{requirements}\n",
    "#Scale:\n",
    "{scale}\n",
    "#Features:\n",
    "{features}\n",
    "\n",
    "Given the requirements the recommended architecture for a solution on AWS.\n",
    "A:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = architect_prompt.format(\n",
    "    requirements=\"test\", \n",
    "    scale=\"testing\", \n",
    "    features=\"Auch was\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but can get a bit clunky when you try to scale it out to more complex use cases. The next type of model wrapper provides a solution to this problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "These models structure their input and outputs with Schemata that enable you to reason about the expected input and output process. This helps to build more complex designs by seperating the inputs used to provide the model with its role instruction, the query and the context to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "chat_llm = ChatAnthropic(model=\"claude-v1\", temperature=0.7, anthropic_api_key=vars['ANTROPIC_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "chat_llm([\n",
    "    SystemMessage(content=\"You are an unhelpful AI bot that makes jokes at whatever the user says.\"),\n",
    "    HumanMessage(content=\"I would like to go to New York, how should i do this?\"),\n",
    "    AIMessage(content=\"???\")\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemata\n",
    "\n",
    "We see that ChatModels use typed classes to structure inputs. This is an example of a LangChain `Schema`, but its just one of many. \n",
    "\n",
    "LangChain currently provides the following schemata:\n",
    "\n",
    "* **Text** The primary interface to interact with a model (used with LanguageModels\n",
    "* **ChatMessages** What you saw we defined up with the ChatModel\n",
    "* **Examples** Input/output pairs acting as context for fine tuning model behavior in n-shot learning\n",
    "* **Document** Piece of unstructured data holding data as content and metadata for retrieval in context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatMessages Schema\n",
    "The primary interface through which end users interact with these is a chat interface. For this reason, some model providers even started providing access to the underlying API in a way that expects chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "hum_msg = HumanMessage(content='inputs send to the model by the user', additional_kwargs={}, example=True)\n",
    "hum_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = SystemMessage(content='Instructions to the model', additional_kwargs={})\n",
    "sys_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg = AIMessage(content='Context answer providing further input to the model', additional_kwargs={})\n",
    "ai_msg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure allows us to simply pass multiple requests into a model for batch processing, making application integration easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate completions for multiple sets of messages\n",
    "batch_messages = [\n",
    "    [   SystemMessage(content=\"You are a helpful assistant that translates English to German.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to German.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Examples\n",
    "These can be inputs/outputs for a model or for a chain. Both types of examples serve a different purpose. Examples for a model can be used to finetune a model. Examples for a chain can be used to evaluate the end-to-end chain, or maybe even train a model to replace that whole chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add code on how to use the examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Documents\n",
    "A piece of unstructured data. Consists of page_content (the content of the data) and metadata (auxiliary pieces of information describing attributes of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add code on how to use Documents "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1\n",
    "We are working to enable our marketing team to provide customized sales emails at scale. You are asked to create to engineer a prompt for a custom marketing email copy creation pipeline. \n",
    "\n",
    "You will be given the following inputs that are collected on the users in your database:\n",
    "* Name \n",
    "* Age\n",
    "* Interest (List of strings)\n",
    "\n",
    "You will also be given a recommended product to personalize-recommend to the user\n",
    "* Product described as a dictionary of attributes (document from DB)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work to complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "#TODO Rewrite for callcenter use case\n",
    "\n",
    "# Complete the function \n",
    "def create_email_copy(name: str, age: int, interests: List[str], product: dict) -> str:\n",
    "    \"\"\"\n",
    "    The email should be personalized, be age appropriate, target the interests \n",
    "    of the person and market the product you are selling. \n",
    "\n",
    "    Fill in this template using string formatting and a combination of the prompt\n",
    "    engineering techniques you have learned previously. \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the product you are selling. Play with the level of detail\n",
    "\n",
    "product = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of users to generate eamils for\n",
    "users = [\n",
    "    {\n",
    "    \"name\": \"\",\n",
    "    \"age\": 0,\n",
    "    \"intesrests\": [],\n",
    "    \"product\": product\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your marketing output\n",
    "for user in users:\n",
    "    print(\"\\n\\n\")\n",
    "    print(llm(create_email_copy(user)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building more complex scenarios, managing the parameters placed into the templates can be too complex for simple string injection methods. Eventually you want to describe the interface in a more programmatic way. Here the `PromptTemplate` helps to define verified input variables to be utilized in the format string.\n",
    "\n",
    "### The PromptTemplate class \n",
    "\n",
    "Let's structure our architecture template to make it reusable in our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# First we can define an exposed parameter interface to the format string\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"requirements\", \"scale\", \"features\"],\n",
    "    template=architect_prompt,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template can be asked to format itself, returning the compiled format string for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = architect_prompt.format(\n",
    "    requirements=\"External facing web application written in Javascript, global deployment\",\n",
    "    scale=\"Average of 500 requests per minute, scale events up to 3000 requests per second\",\n",
    "    features=\"Mobile website, desktop version, javascript\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(final_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a string output is nice and dandy, but what if we want to create structure returns for further use in our applications. For example, how would we continue working with an extracted set of attributes from a text in a parser scenario? \n",
    "\n",
    "Is a string good enough, or would we rather want to return a named tuple, dict or list of class instances from the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=\"\"\"\n",
    "    You identify named entities in the text and extract relations amongst them. \n",
    "    You do not answer questions, and you do not ask questions.\n",
    "    It is very important to extract all references you find. Do not skip any in your output.\n",
    "\n",
    "    # Examples:\n",
    "    The Dow Jones closed with a plus of 1456 points // (\"Dow Jones\", \"closed\", \"1456 points\")\n",
    "\n",
    "    Q: {text} // \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(references(\"Mister Higgins bought the old house next to the woods.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement the ParserClass\n",
    "class ReferenceOutputParser(BaseOutputParser):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchaining "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex workflows will involve the abbility of the model to react to specific subsets of tasks with specialized sequence of behaviors. We would capture these subsections of our application into modules called `chains`. \n",
    "\n",
    "Each chain structures the interaction with a model through specialized prompts, optional examples and optional output parsers. \n",
    "\n",
    "The langchain chains module contains the classes to help us easily create specialized sequences of chains that can receive inputs from previous model inferences as structured outputs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LLMChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "    Create a marketable company name for a company selling {product}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answering multiple questions at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'product': \"Kids kites\"},\n",
    "    {'product': \"Running shoes\"},\n",
    "    {'product': \"Tennis sports wear\"},\n",
    "]\n",
    "\n",
    "res = chain.generate(qs)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a more realistic problem to solve with LLMs that we can not easily solve with a simngle prompt. Here we see the power of chaining a series of steps conducted by an agent using serialized IO of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "    \"\"\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: First create name of the company, then provide a slogan, then create a mission and vision statement\n",
    "\n",
    "slogan_prompt = PromptTemplate(\n",
    "    input_variables=['product', 'company_name'],\n",
    "    template=\"\"\"\n",
    "    You are desigining a corporate identity for {company_name}.\n",
    "    The company is selling {product} and wants to be \n",
    "    \"\"\"\n",
    ")\n",
    "# Slogan chain\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "Let's bring it all together to create a custom LangChain that can build up a cooking article for our new online magazine. \n",
    "\n",
    "We will develop a series of chains to expand on a set of initial travel destinations to cover. \n",
    "\n",
    "Each article will contain:\n",
    "* A overview section of the travel destination\n",
    "* A list of things to do in the region\n",
    "* A recipe for a famous local dish (description, list of ingredients, step by step instruction)\n",
    "\n",
    "### Target\n",
    "Your system should create the articles based on the following inputs:\n",
    "\n",
    "* Travel destination (City, Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by creating the subchains\n",
    "\n",
    "# 1. Create the destination_overview chain\n",
    "\n",
    "\n",
    "# 2. Create the recommended_activities chain\n",
    "\n",
    "\n",
    "# 3. Create the famous_local_dish recommender \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the chains should be constructed for a subset of the workflow that is self contained. Extending too many tasks at once can overload the model and lead to poor results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now unifiy the elements and ensure the information flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of locations to cover in your reports\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the chain against the location_list\n",
    "result = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have seen how LangChain provides us with a series of abstractions on the core building blocks of LLM based applications. \n",
    "\n",
    "We have learned to connect our models, create custom Templates, use Schemata to structure our message flow, chain a series of steps and structure the models outputs to further pass into downstream systems. \n",
    "\n",
    "Each component of the system is under continued development and likely you will see the library change as it continues to mature. \n",
    "\n",
    "Still, we believe LangChain to be a useful abstraction to develop faster and build a more sustainable code base. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

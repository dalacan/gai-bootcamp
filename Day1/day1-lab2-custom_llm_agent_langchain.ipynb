{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c97b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet --disable-pip-version-check --root-user-action=ignore\n",
    "!pip install langchain==0.0.206 --quiet --disable-pip-version-check --root-user-action=ignore\n",
    "!pip install openai --quiet --disable-pip-version-check --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2880903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, boto3, json\n",
    "from typing import List, Union\n",
    "import nltk\n",
    "from sagemaker import Session\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f8741",
   "metadata": {},
   "source": [
    "# Custom LLM Agent\n",
    "\n",
    "This notebook goes through how to create your own custom LLM agent.\n",
    "\n",
    "An LLM agent consists of three parts:\n",
    "\n",
    "- PromptTemplate: This is the prompt template that can be used to instruct the language model on what to do\n",
    "- LLM: This is the language model that powers the agent\n",
    "- `stop` sequence: Instructs the LLM to stop generating as soon as this string is found\n",
    "- OutputParser: This determines how to parse the LLMOutput into an AgentAction or AgentFinish object\n",
    "\n",
    "\n",
    "The LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:\n",
    "1. Passes user input and any previous steps to the Agent (in this case, the LLMAgent)\n",
    "2. If the Agent returns an `AgentFinish`, then return that directly to the user\n",
    "3. If the Agent returns an `AgentAction`, then use that to call a tool and get an `Observation`\n",
    "4. Repeat, passing the `AgentAction` and `Observation` back to the Agent until an `AgentFinish` is emitted.\n",
    "    \n",
    "`AgentAction` is a response that consists of `action` and `action_input`. `action` refers to which tool to use, and `action_input` refers to the input to that tool. `log` can also be provided as more context (that can be used for logging, tracing, etc).\n",
    "\n",
    "`AgentFinish` is a response that contains the final message to be sent back to the user. This should be used to end an agent run.\n",
    "        \n",
    "In this notebook we walk through how to create a custom LLM agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4812c",
   "metadata": {},
   "source": [
    "## Set up environment\n",
    "\n",
    "Do necessary imports, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ca8055-c4ff-4e1e-ac41-de09ad771050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface-llm-falcon-40b-instruct-bf16\n",
      "huggingface-textembedding-gpt-j-6b-fp16\n",
      "You did not load the AI21 Jurassic Grande model and will not be able to select it for this lab\n"
     ]
    }
   ],
   "source": [
    "#load stored variables from previous notebook\n",
    "%store -r\n",
    "\n",
    "# Initialize key environment variables\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "model_version = \"*\"\n",
    "\n",
    "print(falcon_inference_model)\n",
    "# print(flant5_inference_model)\n",
    "print(embedding_model)\n",
    "# print(falcon_inference_model)\n",
    "try:\n",
    "    print(ai21_inference_model)\n",
    "except:\n",
    "    print(\"You did not load the AI21 Jurassic Grande model and will not be able to select it for this lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcd18d",
   "metadata": {},
   "source": [
    "### Registering your Third-Party API key (PURELY OPTIONAL!)\n",
    "We will be running a section on the ChatModel API exposed by a series of API endpoint providers such as OpenAI, Anthropic, Google Vertex. As this is currently not supported by the SageMaker deployed models, you can choose to experimment with at your own costs if you register for an OpenAI key, or you have previous access to Anthropic (as they are currently not accepting new registrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf70dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key=\"\"\n",
    "anthropic_api_key=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820a9e6",
   "metadata": {},
   "source": [
    "#### Load Widgets used across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2736d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Select, Text\n",
    "\n",
    "# This creates the widgets used across the notebook for easier configuration\n",
    "model_selections = [\n",
    "    # 'Flan-t5-xxl',\n",
    "    'SageMaker-Falcon40B', \n",
    "    # 'AI21-Jurrasic-Grande'\n",
    "]\n",
    "# Subset based on available ApiKeys\n",
    "if openai_api_key:\n",
    "    model_selections.append('OpenAI')\n",
    "if anthropic_api_key:\n",
    "    model_selections.append('Anthropic-Claude')\n",
    "\n",
    "model_selection_widget = Select(\n",
    "    options=model_selections\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30fc1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(failure) - You have not provided an OpenAPI key, and you won't have access to work with the model in this notebook\n",
      "(failure) - You have not provided an AnthropicAPI key, and you won't have access to work with the model in this notebook\n"
     ]
    }
   ],
   "source": [
    "# Connecting to Third-party endpoints using provided API keys \n",
    "from langchain import OpenAI\n",
    "\n",
    "if openai_api_key:\n",
    "    openai_llm = OpenAI(openai_api_key=openai_api_key)\n",
    "    print(\"(success) - Successfully connected to OpenAI\")\n",
    "else:\n",
    "    print(\"(failure) - You have not provided an OpenAPI key, and you won't have access to work with the model in this notebook\")\n",
    "\n",
    "# Work with Anthropic\n",
    "from langchain import Anthropic\n",
    "\n",
    "if anthropic_api_key:\n",
    "    anthropic_llm = Anthropic(anthropic_api_key=anthropic_api_key)\n",
    "    print(\"(success) - Successfully connected to Anthropic\")\n",
    "else:\n",
    "    print(\"(failure) - You have not provided an AnthropicAPI key, and you won't have access to work with the model in this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237ae8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing reuqired dependencies for third party Foundation APIs\n",
    "import sys\n",
    "if openai_api_key:\n",
    "    !pip install openai --quiet\n",
    "if anthropic_api_key:\n",
    "    !pip install anthropic --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e2e696",
   "metadata": {},
   "source": [
    "## Connecting your model on AWS SageMaker\n",
    "To work with your models on AWS you can use either an integration with the SageMaker endpoint, or in the future directly talk to the Bedrock API. \n",
    "\n",
    "For now, let's look at how to work with a custom SageMaker Model Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b38a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for FlanT5 model \n",
    "flant5_parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}\n",
    "# Model configuration for falcon 40b instruct\n",
    "falcon_parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}\n",
    "\n",
    "# Configuration for Jurassic Grande Model\n",
    "j2_grande_parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09da5880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model endpoint: huggingface-llm-falcon-40b-instruct-bf16\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "class InferenceContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "inference_content_handler = InferenceContentHandler()\n",
    "\n",
    "# Instantiate all available models\n",
    "# flant5_xxl_llm = SagemakerEndpoint(\n",
    "#     endpoint_name=_MODEL_CONFIG_[flant5_inference_model]['endpoint_name'],\n",
    "#     region_name=aws_region,\n",
    "#     model_kwargs=flant5_parameters,\n",
    "#     content_handler=inference_content_handler,\n",
    "#     )\n",
    "# print(f\"Loaded model endpoint: {flant5_inference_model}\")\n",
    "\n",
    "falcon_sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=_MODEL_CONFIG_[falcon_inference_model]['endpoint_name'],\n",
    "    region_name=aws_region,\n",
    "    model_kwargs=falcon_parameters,\n",
    "    content_handler=inference_content_handler,\n",
    "    )\n",
    "print(f\"Loaded model endpoint: {falcon_inference_model}\")\n",
    "\n",
    "# Optional load\n",
    "# j2_grande_sm_llm = SagemakerEndpoint(\n",
    "#     endpoint_name=_MODEL_CONFIG_[ai21_inference_model]['endpoint_name'],\n",
    "#     region_name=aws_region,\n",
    "#     model_kwargs=j2_grande_parameters,\n",
    "#     content_handler=inference_content_handler,\n",
    "#     )\n",
    "# print(f\"Loaded model endpoint: {ai21_inference_model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59028190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529a08b1d3904944b375a3c68198cd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(options=('SageMaker-Falcon40B',), value='SageMaker-Falcon40B')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b22865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated SageMaker-Falcon40B\n"
     ]
    }
   ],
   "source": [
    "match model_selection_widget.value:\n",
    "    case \"Flan-t5-xxl\":\n",
    "        llm = flant5_xxl_llm \n",
    "    case \"SageMaker-Falcon40B\":\n",
    "        llm = falcon_sm_llm\n",
    "    case \"AI21-Jurrasic-Grande\":\n",
    "        llm = j2_grande_sm_llm\n",
    "    case \"OpenAI\":\n",
    "        llm = openai_llm\n",
    "        \n",
    "print(f\"Activated {model_selection_widget.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2278efb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model endpoint: huggingface-llm-falcon-40b-instruct-bf16\n"
     ]
    }
   ],
   "source": [
    "# Set model configuration\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}\n",
    "\n",
    "class InferenceContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        nltk_tokens = nltk.word_tokenize(prompt)\n",
    "        print(f'[INFO] - Nb of tokens for prompt/context:{len(prompt)}\\n')\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "inference_content_handler = InferenceContentHandler()\n",
    "\n",
    "# Instantiate all available models \n",
    "falcon_sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=_MODEL_CONFIG_[falcon_inference_model]['endpoint_name'],\n",
    "    region_name=aws_region,\n",
    "    model_kwargs=falcon_parameters,\n",
    "    content_handler=inference_content_handler,\n",
    "    )\n",
    "print(f\"Loaded model endpoint: {falcon_inference_model}\")\n",
    "\n",
    "# flant5_xxl_llm = SagemakerEndpoint(\n",
    "#     endpoint_name=_MODEL_CONFIG_[flant5_inference_model]['endpoint_name'],\n",
    "#     region_name=aws_region,\n",
    "#     model_kwargs=flant5_parameters,\n",
    "#     content_handler=inference_content_handler,\n",
    "#     )\n",
    "# print(f\"Loaded model endpoint: {flant5_inference_model}\")\n",
    "\n",
    "# Optional load\n",
    "# j2_grande_sm_llm = SagemakerEndpoint(\n",
    "#     endpoint_name=_MODEL_CONFIG_[ai21_inference_model]['endpoint_name'],\n",
    "#     region_name=aws_region,\n",
    "#     model_kwargs=j2_grande_parameters,\n",
    "#     content_handler=inference_content_handler,\n",
    "#     )\n",
    "# print(f\"Loaded model endpoint: {ai21_inference_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0253f",
   "metadata": {},
   "source": [
    "## Set up tools\n",
    "\n",
    "Set up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71666d-7d3c-4bee-9978-6efad04f5a7d",
   "metadata": {},
   "source": [
    "SERAPI is a webscraper for Google and other search engines that provides api to search for the content.\n",
    "\n",
    "register here for 30 days free trial:\n",
    "https://serpapi.com/users/sign_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09dd351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b810fa9-82c2-4a33-b89c-09b89f256ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"] = \"<insert your key here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "becda2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define which tools the agent can use to answer user queries\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a075c",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "This instructs the agent on what to do. Generally, the template should incorporate:\n",
    "    \n",
    "- `tools`: which tools the agent has access and how and when to call them.\n",
    "- `intermediate_steps`: These are tuples of previous (`AgentAction`, `Observation`) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way.\n",
    "- `input`: generic user input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d6940-7fb8-4bca-8c29-05fda8375586",
   "metadata": {},
   "source": [
    "### We're adding few shots learning for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d78588-a041-442d-8268-d3efde2cc4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_shots=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d619bb-f8e0-4066-a0d2-406e392d0002",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfew_shots = \"\"\"Answer the following questions as best you can. You have access to the following tools:\\n\\n\"\"\"\\nfew_shots += \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\nfew_shots += \"\"\"\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: What profession does Nicholas Ray and Elia Kazan have in common?\\nThought: They are both filmmakers.\\nAction: Search\\nAction Input: \"Nicholas Ray\" \"Elia Kazan\" profession\\n\\nObservation:Nicholas Ray was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause. He is appreciated for many narrative ...\\n Elia Kazan was also a film director, screenwriter, and actor.\\nAction: Search\\nAction Input: \"Elia Kazan\" profession\\n\\nObservation:Elia Kazan was an American film and theatre director, producer, screenwriter and actor, described by The New York Times as \"one of the most honored and influential directors in Broadway and Hollywood history\". Born in Constantinople, to Cappadocian Greek parents, his family came to the United States in 1913.\\n They both have the same profession of film director, screenwriter, and actor.\\nFinal Answer: Nicholas Ray and Elia Kazan have the same profession of film director, screenwriter, and actor.\\n\\n\\n\"\"\"\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "few_shots = \"\"\"Answer the following questions as best you can. You have access to the following tools:\\n\\n\"\"\"\n",
    "few_shots += \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "few_shots += \"\"\"\\n\\nUse the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
    "Thought: They are both filmmakers.\n",
    "Action: Search\n",
    "Action Input: \"Nicholas Ray\" \"Elia Kazan\" profession\n",
    "\n",
    "Observation:Nicholas Ray was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause. He is appreciated for many narrative ...\n",
    " Elia Kazan was also a film director, screenwriter, and actor.\n",
    "Action: Search\n",
    "Action Input: \"Elia Kazan\" profession\n",
    "\n",
    "Observation:Elia Kazan was an American film and theatre director, producer, screenwriter and actor, described by The New York Times as \"one of the most honored and influential directors in Broadway and Hollywood history\". Born in Constantinople, to Cappadocian Greek parents, his family came to the United States in 1913.\n",
    " They both have the same profession of film director, screenwriter, and actor.\n",
    "Final Answer: Nicholas Ray and Elia Kazan have the same profession of film director, screenwriter, and actor.\n",
    "\\n\\n\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd69312-5501-46f5-9ccd-534f4bc7acfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfew_shots += \"\"\"Answer the following questions as best you can. You have access to the following tools:\\n\\n\"\"\"\\nfew_shots += \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\\nfew_shots += \"\"\"\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\\nThought: Matt Groening named Milhouse after someone.\\nThought: I need to find out who that is.\\nAction: Search\\nAction Input: \"Matt Groening\" \"Milhouse\"\\n\\nObservation:For other uses, see Millhouse (disambiguation) and Milhous (disambiguation). Milhouse Mussolini Van Houten is a recurring character in the Fox animated ...\\n I need to find out who Matt Groening named Milhouse after.\\nAction: Search\\nAction Input: \"Matt Groening\" \"Milhouse\" origin\\n\\nObservation:Creation. Milhouse was designed by Matt Groening for a planned series on NBC, which was abandoned. The design was then used for a Butterfinger commercial, and it was decided to use the character in the series. Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\\n I now know the final answer.\\nFinal Answer: Matt Groening named the character Milhouse after U.S. president Richard Nixon, whose middle name was Milhous.\\n\\n\"\"\"\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "few_shots += \"\"\"Answer the following questions as best you can. You have access to the following tools:\\n\\n\"\"\"\n",
    "few_shots += \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])\n",
    "few_shots += \"\"\"\\n\\nUse the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "Thought: Matt Groening named Milhouse after someone.\n",
    "Thought: I need to find out who that is.\n",
    "Action: Search\n",
    "Action Input: \"Matt Groening\" \"Milhouse\"\n",
    "\n",
    "Observation:For other uses, see Millhouse (disambiguation) and Milhous (disambiguation). Milhouse Mussolini Van Houten is a recurring character in the Fox animated ...\n",
    " I need to find out who Matt Groening named Milhouse after.\n",
    "Action: Search\n",
    "Action Input: \"Matt Groening\" \"Milhouse\" origin\n",
    "\n",
    "Observation:Creation. Milhouse was designed by Matt Groening for a planned series on NBC, which was abandoned. The design was then used for a Butterfinger commercial, and it was decided to use the character in the series. Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    " I now know the final answer.\n",
    "Final Answer: Matt Groening named the character Milhouse after U.S. president Richard Nixon, whose middle name was Milhous.\\n\\n\"\"\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "339b1bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"{few_shots}\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd969d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    few_shots: str\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "            \n",
    "        #Set the few shots variable\n",
    "        kwargs[\"few_shots\"] = self.few_shots\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "\n",
    "        #print(self.template.format(**kwargs))\n",
    "        return self.template.format(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "798ef9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    few_shots=few_shots,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a1af3",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "The output parser is responsible for parsing the LLM output into `AgentAction` and `AgentFinish`. This usually depends heavily on the prompt used.\n",
    "\n",
    "This is where you can change the parsing to do retries, handle whitespace, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c6fe0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        #print(f'llm_output={llm_output}')\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        #regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)Action\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d278706a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170587b1",
   "metadata": {},
   "source": [
    "## Set up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5efa1b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529a08b1d3904944b375a3c68198cd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(options=('SageMaker-Falcon40B',), value='SageMaker-Falcon40B')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_selection_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9d4c374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated SageMaker-Falcon40B\n"
     ]
    }
   ],
   "source": [
    "# Loading the selected model\n",
    "match model_selection_widget.value:\n",
    "    case \"SageMaker-Falcon40B\":\n",
    "        llm = falcon_sm_llm\n",
    "    case \"OpenAI\":\n",
    "        llm = openai_llm\n",
    "        \n",
    "print(f\"Activated {model_selection_widget.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeab5e4",
   "metadata": {},
   "source": [
    "## Define the stop sequence\n",
    "\n",
    "This is important because it tells the LLM when to stop generation.\n",
    "\n",
    "This depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an `Observation` (otherwise, the LLM may hallucinate an observation for you)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be9f65",
   "metadata": {},
   "source": [
    "## Set up the Agent\n",
    "\n",
    "We can now combine everything to set up our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b1cc2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4f5092f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_names=['Search']\n"
     ]
    }
   ],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "print(f'tool_names={tool_names}')\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a5326",
   "metadata": {},
   "source": [
    "## Use the Agent\n",
    "\n",
    "Now we can use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "490604e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "819937fc-1064-4441-8bed-df37656c0c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#question = \"\"\"Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\"\"\"\n",
    "#question= \"How many people live in canada as of 2023?\"\n",
    "#question = \"What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\"\n",
    "question = \"What profession does Nicholas Ray and Elia Kazan have in common?\"\n",
    "#question= \"who won the roland garros tennis tournament in 2023?\"\n",
    "#question= \"Were Pavel Urysohn and Leonid Levin known for the same type of work?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b1617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4a078",
   "metadata": {},
   "source": [
    "## Adding Memory\n",
    "\n",
    "If you want to add memory to the agent, you'll need to:\n",
    "\n",
    "1. Add a place in the custom prompt for the chat_history\n",
    "2. Add a memory object to the agent executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fffda1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template_with_history = \"\"\"{few_shots}\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "New question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58488d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_with_history = CustomPromptTemplate(\n",
    "    template=template_with_history,\n",
    "    tools=tools,\n",
    "    few_shots=few_shots,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d4b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37b32a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea1bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad69ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory=ConversationBufferWindowMemory(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5c9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4c39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.run(\"How many people live in canada as of 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba45bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.run(\"how about in mexico?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd820a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

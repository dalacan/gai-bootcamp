{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d2ede-0d26-4ad4-bf62-55073d866244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This notebook has been tested with Conda Pytorch Python 3.10 kernel on a ml.g5.24xlarge instance, you may be able to go smaller to a ml.g5.12xlarge\n",
    "\n",
    "#!pip install transformers bitsandbytes git+https://github.com/huggingface/peft sentencepiece datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f715e2-4ae8-4b15-a085-6ba4fd334a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Do this so that your cache directory does not fill up, there is a Sagemaker limit of 5Gb\n",
    "\n",
    "!export TRANSFORMERS_CACHE=/home/ec2-user/SageMaker/\n",
    "!export HF_DATASETS_CACHE=/home/ec2-user/SageMaker/\n",
    "!export HF_HOME=/home/ec2-user/SageMaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b90a8c-459b-46d3-a670-3f79964d717f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, LoraModel\n",
    "\n",
    "def create_model(huggingface_id, tokenizer_id=None, four_bit=True, use_cpu=False):\n",
    "    if tokenizer_id is None:\n",
    "        tokenizer_id = huggingface_id\n",
    "\n",
    "    #Convert the model to Magic nf4 format\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    if not four_bit:\n",
    "        bnb_config = None\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    device_map = {\"\":0}\n",
    "    if use_cpu:\n",
    "        device_map = {\"\": \"cpu\"}\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(huggingface_id, quantization_config=bnb_config, \\\n",
    "                                                 device_map=device_map,trust_remote_code=True)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_lora(model, lora_path):\n",
    "    model = PeftModel.from_pretrained(model, lora_path)\n",
    "    return model\n",
    "    \n",
    "def loraify_model(model, rank=8, alpha=32, dropout=0.05):\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    config = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=alpha,\n",
    "        # Add a lora adapter to the par \n",
    "        target_modules=[\"q_proj\", \"v_proj\"], # For LLaMA\n",
    "        # target_modules=[\"query_key_value\", \"xxx\"], # For RedPajama\n",
    "        lora_dropout=dropout, \n",
    "        bias=\"none\", \n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, tokenizer, dataset, lora_name, batch_size=4, gradient_accum=4, max_steps=10000, save_steps=200, logging_steps=5, num_train_epochs=10):    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=gradient_accum,\n",
    "            warmup_steps=2,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=logging_steps,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            save_steps=save_steps,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    trainer.train()\n",
    "    model.save_pretrained(lora_name)\n",
    "    \n",
    "def generate_completion(model, tokenizer, prompt, max_length):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c1475-1e28-4526-bd39-e3094cf4446e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare a model for Q-Lora format by attaching an adapter\n",
    "model, tokenizer = create_model(\"openlm-research/open_llama_7b\")\n",
    "model = loraify_model(model, rank=8, alpha=16, dropout=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28255fc4-6f65-4b56-8771-6e486c663b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#Load your instruction dataset here\n",
    "file_name=\"instruction_data.json\"\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": file_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd245a34-0487-42b5-b6ba-457428ad0abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#take a peek\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924fdb9-e447-4bbf-87c4-ade4881c6970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Merge the intruction, input and output columns. Once merged tokenize and drop old columns\n",
    "\n",
    "def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"instruction\"] + examples[\"input\"] + examples[\"output\"], \\\n",
    "                         padding=\"max_length\", truncation=True, max_length=256)\n",
    "dataset = dataset.map(tokenize_function, batched=True, remove_columns=['instruction', 'output', 'input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f8a37-7ca9-474a-a4c9-583e48350494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32e5e8-456c-4399-92e1-21d10e85f0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e625b-dead-41d0-95cd-d454e79b4084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Train your Q-Lora model\n",
    "train_model(model, tokenizer, dataset, \"open_llama_chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081d0d2-237f-4d0b-8230-110bb8522168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load you intial model plus Q-Lora adapter\n",
    "model, tokenizer = create_model(\"openlm-research/open_llama_7b\")\n",
    "model = load_lora(model, \"open_llama_chat\")\n",
    "completion = generate_completion(model, tokenizer, \"LoRA is\", 100)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023f6b3-0861-42fb-921b-201d68718ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
